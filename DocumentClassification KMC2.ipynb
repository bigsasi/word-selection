{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleccionando palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import reuters_reader\n",
    "from kmc2 import kmc2\n",
    "from collections import Counter\n",
    "from sklearn.externals import joblib\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from copy import deepcopy\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCV1 Dataset\n",
    "\n",
    "Use the function ```reuters_reader.reader(path)``` to retrieve the available documents from the rcv1 dataset stored in `path`. This function returns a generator (```reader```) which yields a single document each time we call ```next(reader)```. Each document is a dictionary with the followitn useful keys:\n",
    " - \"title\" is the title of the document\n",
    " - \"text\" is the body of the document\n",
    " - \"bip:topics:1.0\" is the list of topics\n",
    " \n",
    "There are a total of 804420 available documents, although some may have no topic.\n",
    "\n",
    "#### Building the dataset\n",
    "We build a balanced dataset that contains ```n_docs```. To get a balanced dataset we iterate through the documents generator until we have ```n_docs / 2``` documents with the desired topic and the same amount without it. \n",
    "\n",
    "#### Get the labels\n",
    "Select a topic we want to classify using the variable topic (there is a list of the topics https://gist.github.com/gavinmh/6253739). Then build the list of labels using a 1 for those documents with that topic and 0 otherwise\n",
    "\n",
    "#### Training and validation set\n",
    "Finally, we split the dataset using the ```train_split``` value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19961028 / 146544newsML.xml failed to parse XML.\n",
      "19970601 / 629003newsML.xml failed to parse XML.\n",
      "19970725 / 756041newsML.xml failed to parse XML.\n",
      "50000 docs with topic GCAT (from 100000)\n",
      "Training with 80000 docs\n",
      "Validating with 20000 docs\n"
     ]
    }
   ],
   "source": [
    "path = 'rcv1'\n",
    "n_docs = 100000\n",
    "train_split = 0.8\n",
    "topic = 'GCAT'\n",
    "\n",
    "docs = []\n",
    "reader = reuters_reader.reader(path)\n",
    "\n",
    "topic_true = 0\n",
    "topic_false = 0\n",
    "\n",
    "while len(docs) < n_docs:\n",
    "    doc = next(reader)\n",
    "    if topic in doc['bip:topics:1.0']:\n",
    "        topic_true += 1\n",
    "        if topic_true <= n_docs // 2:\n",
    "            docs.append(doc)\n",
    "    else:\n",
    "        topic_false += 1\n",
    "        if topic_false <= n_docs // 2:\n",
    "            docs.append(doc)\n",
    "     \n",
    "random.shuffle(docs)\n",
    "\n",
    "labels = np.zeros((n_docs), dtype=np.int16)\n",
    "labels = [1 if topic in doc['bip:topics:1.0'] else 0 for doc in docs]\n",
    "\n",
    "print('{} docs with topic {} (from {})'.format(np.sum(labels), topic, n_docs))\n",
    "\n",
    "split_point = int(n_docs * train_split)\n",
    "x_train, y_train = docs[:split_point], labels[:split_point]\n",
    "x_val, y_val = docs[split_point:], labels[split_point:]\n",
    "\n",
    "print('Training with {} docs'.format(len(x_train)))\n",
    "print('Validating with {} docs'.format(len(x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec model\n",
    "\n",
    "We are loading the well known word2vec model from __[Google](https://code.google.com/archive/p/word2vec/)__ which is stored in the binary file `GoogleNews-vectors-negative300.bin`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_name = 'GoogleNews-vectors-negative300.bin'\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_name, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our vocabulary\n",
    "\n",
    "Get all the the vectors from the word2vec for our vocabulary. Our vocabulary can include all the words used in the word2vec model or be limited to the words in our dataset.\n",
    "\n",
    "We can change this behaviour with the flag ```dataset_vocabulary```. ```False``` will use all the words from the word2vec model and ```True``` will limit them to just the words that are in our dataset and in the model at the same time.\n",
    "\n",
    "There is a ```count_threshold``` to remove those words appearing very few times because they are probably errors.\n",
    "\n",
    "As we have to split each documents in individual words, we already save this inside each document with the key \"counter\".\n",
    "\n",
    "After this cell, ```X``` is a matrix including all the vectors we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 62100\n",
      "Vector length: 300\n"
     ]
    }
   ],
   "source": [
    "dataset_vocabulary = True\n",
    "count_threshold = 5\n",
    "\n",
    "if dataset_vocabulary:\n",
    "    vocab = Counter()\n",
    "    for doc in docs:\n",
    "        doc[\"counter\"] = Counter()\n",
    "        doc[\"word_count\"] = 0\n",
    "        words = doc[\"text\"].split()\n",
    "        words = [word.strip(string.punctuation) for word in words]\n",
    "        for word in words:\n",
    "            if word in w2v:\n",
    "                doc[\"counter\"][word] += 1\n",
    "                doc[\"word_count\"] += 1\n",
    "                vocab[word] += 1\n",
    "    vocab = {word: count for word, count in vocab.items() if count > count_threshold}\n",
    "    vocab_array = np.array(list(vocab))\n",
    "else:\n",
    "    vocab = w2v.index2word\n",
    "\n",
    "X = np.zeros((len(vocab), w2v.vector_size), dtype=np.float32)\n",
    "for index, word in enumerate(vocab):\n",
    "    X[index, :] += w2v[word]\n",
    "    \n",
    "print(\"Vocabulary length: {}\".format(X.shape[0]))\n",
    "print(\"Vector length: {}\".format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = dict(enumerate(vocab))\n",
    "indptr = [0]\n",
    "indices = []\n",
    "data = []\n",
    "\n",
    "idf_docs = Counter()\n",
    "for doc in docs:\n",
    "    for word in doc[\"counter\"]:\n",
    "        idf_docs[word] += 1\n",
    "\n",
    "for doc in docs:\n",
    "    max_f = doc[\"counter\"].most_common(1)[0][1]\n",
    "    for word in doc[\"counter\"]:\n",
    "        if word in word2idx:\n",
    "            indices.append(word2idx[word])\n",
    "            tf = doc[\"counter\"][word] / max_f\n",
    "            idf = np.log(n_docs / idf_docs[word])\n",
    "            data.append(tf * idf)\n",
    "    indptr.append(len(indices))\n",
    "    \n",
    "matrix = csr_matrix((data, indices, indptr), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_point = int(n_docs * train_split)\n",
    "# baseline_x_train, baseline_y_train = matrix[:split_point], labels[:split_point]\n",
    "# baseline_x_val, baseline_y_val = matrix[split_point:], labels[split_point:]\n",
    "\n",
    "# lasso = fit_lasso(baseline_x_train, baseline_y_train)\n",
    "# validate(lasso, baseline_x_val, baseline_y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shares',\n",
       " 'Revs',\n",
       " 'DATE',\n",
       " 'tonnes',\n",
       " 'Year',\n",
       " 'Quarterly',\n",
       " 'stocks',\n",
       " 'Prior',\n",
       " 'Record',\n",
       " 'Avg',\n",
       " 'profit',\n",
       " 'Latest',\n",
       " 'futures',\n",
       " 'Pay',\n",
       " 'rating',\n",
       " 'Ended',\n",
       " 'price',\n",
       " 'loss',\n",
       " 'Co',\n",
       " 'rose',\n",
       " 'traders',\n",
       " 'net',\n",
       " 'Total',\n",
       " 'NOTE',\n",
       " 'company',\n",
       " 'prices',\n",
       " 'stock',\n",
       " 'bonds',\n",
       " 'Income',\n",
       " 'sales',\n",
       " 'Shr',\n",
       " 'police',\n",
       " 'Ltd',\n",
       " 'income',\n",
       " 'stories',\n",
       " 'index',\n",
       " 'pct',\n",
       " 'percent',\n",
       " 'Amount',\n",
       " 'cents',\n",
       " 'market',\n",
       " 'his',\n",
       " 'million',\n",
       " 'yen',\n",
       " 'Inc',\n",
       " 'per',\n",
       " 'share',\n",
       " 'shares',\n",
       " 'vs',\n",
       " 'Net']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "scores, _ = chi2(matrix, labels)\n",
    "sorted_idx = np.argsort(scores, kind=\"mergesort\")[-50:]\n",
    "words = [idx2word[idx] for idx in sorted_idx]\n",
    "initial_centers = np.array([w2v[word] for word in words])\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the most common and uncommon words in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1244626), ('in', 526190), ('said', 330822), ('on', 294388), ('for', 233096), ('The', 208167), ('that', 164547), ('was', 163462), ('at', 151405), ('is', 150826)]\n",
      "[('stampers', 6), ('Schwertfeger', 6), ('CACL', 6), ('supertasters', 6), ('Ozimek', 6), ('SARU', 6), ('Kometani', 6), ('blockers', 6), ('Balestrino', 6), ('ISESCO', 6)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(vocab).most_common()[:10])\n",
    "print(Counter(vocab).most_common()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Cluster\n",
    "\n",
    "Train the first Kmeans cluster using the complete set of words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 50\n",
    "# kmeans_name = 'kmeans' + str(n_clusters) + '.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "class Kmeans:\n",
    "\n",
    "    def __init__(self, n_clusters):\n",
    "        self.cluster_centers_ = []\n",
    "        self.labels_ = []\n",
    "        self.n_clusters = n_clusters\n",
    "        self.kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=0, compute_labels=True)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.kmeans.fit(X)\n",
    "        self.cluster_centers_ = kmc2(X, self.n_clusters)\n",
    "        self.update(self.cluster_centers_, X)\n",
    "    \n",
    "    def update(self, new_centers, X):\n",
    "        self.cluster_centers_ = self.kmeans.cluster_centers_ = new_centers\n",
    "        self.n_clusters = len(new_centers)\n",
    "        self.labels_, _ = self.kmeans._labels_inertia_minibatch(X)\n",
    "        self.kmeans.labels_ = self.labels_\n",
    "        self.counts_ = self.kmeans.counts_ = np.zeros(self.n_clusters, dtype=np.int32)\n",
    "        for i in range(self.n_clusters):\n",
    "            self.counts_[i] = np.sum(self.labels_ == i)\n",
    "            self.kmeans.counts_[i] = np.sum(self.kmeans.labels_ == i)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.kmeans.predict(x)\n",
    "            \n",
    "        \n",
    "kmeans = Kmeans(n_clusters=n_clusters)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print each cluster\n",
    "\n",
    "To have an idea of the clusters we are workings with we can print some of their neighbords. The option selected here is to print the most representative neighbords (those that appear more times in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Liberal', 874), ('Chretien', 331), ('Liberals', 168), ('Axworthy', 110), ('Mulroney', 63)]\n",
      "[('barrels', 2418), ('refinery', 1543), ('Petroleum', 1531), ('bpd', 1375), ('pipeline', 1225)]\n",
      "[('MDL', 12), ('demurrage', 11), ('McMoRan', 6)]\n",
      "[('War', 2700), ('Digital', 355), ('Pen', 236), ('Extraordinary', 234), ('Book', 233)]\n",
      "[('Securities', 6042), ('property', 3343), ('land', 3137), ('Holdings', 2313), ('Investment', 2049)]\n",
      "[('analyst', 5205), ('Research', 1822), ('Results', 1293), ('Statistics', 1074), ('Latest', 996)]\n",
      "[('Conrail', 495), ('highway', 476), ('Rt', 256), ('Highway', 114), ('Interstate', 87)]\n",
      "[('Instruments', 176), ('Controls', 58), ('sensors', 50), ('detectors', 36), ('Components', 28)]\n",
      "[('setting', 1714), ('creating', 981), ('dealing', 870), ('critics', 638), ('ignored', 483)]\n",
      "[('pesos', 2058), ('baht', 1416), ('ringgit', 1249), ('Philippine', 1237), ('rupiah', 1196)]\n",
      "[('weapons', 1940), ('guns', 428), ('artillery', 387), ('ammunition', 297), ('weapon', 247)]\n",
      "[('British', 12083), ('German', 10280), ('Russian', 6800), ('Italian', 4273), ('Yeltsin', 3203)]\n",
      "[('election', 9302), ('elections', 7213), ('vote', 6348), ('polls', 2396), ('voters', 2368)]\n",
      "[('lung', 359), ('breast', 342), ('brain', 321), ('prostate', 220), ('implants', 214)]\n",
      "[('ASEAN', 1182), ('Cambodia', 956), ('Burma', 898), ('Zimbabwe', 855), ('Sudan', 647)]\n",
      "[('forecast', 5327), ('forecasts', 2116), ('economists', 1932), ('Economists', 743), ('rainfall', 334)]\n",
      "[('earthquake', 371), ('quake', 208), ('tremor', 99), ('earthquakes', 89), ('landslides', 70)]\n",
      "[('chase', 163), ('suitors', 26), ('peloton', 10), ('pretenders', 8), ('pursuers', 7)]\n",
      "[('shekels', 703), ('Aviv', 551), ('shekel', 237), ('Yitzhak', 211), ('Likud', 188)]\n",
      "[('U.S', 39459), ('countries', 11643), ('China', 10364), ('yen', 9919), ('world', 9361)]\n",
      "[('difficult', 3043), (\"I'm\", 2565), ('different', 2312), ('bad', 2024), ('tight', 1663)]\n",
      "[('total', 8709), ('compared', 7179), ('recorded', 1184), ('accounting', 1142), ('offset', 950)]\n",
      "[('said', 330822), ('not', 84663), ('people', 23025), ('added', 16390), ('statement', 15401)]\n",
      "[('budget', 11554), ('plans', 10317), ('plan', 8967), ('announced', 8113), ('agreed', 7729)]\n",
      "[('Eurostat', 121), ('Ipsos', 31), ('Eurobarometer', 9)]\n",
      "[('Mobutu', 1782), ('businessmen', 847), ('aides', 572), ('servants', 492), ('speculators', 415)]\n",
      "[('Holyfield', 106), ('Boxing', 102), ('boxing', 98), ('WBC', 34), ('boxer', 26)]\n",
      "[('are', 78020), ('or', 38329), ('more', 37430), ('about', 36698), ('A', 29500)]\n",
      "[('percent', 92059), ('billion', 33160), ('per', 19695), ('prices', 17533), ('rate', 15168)]\n",
      "[('mass', 1356), ('Catholic', 1136), ('funeral', 336), ('pilgrims', 323), ('Mass', 252)]\n",
      "[('Diario', 39), ('Espanol', 22), ('Vive', 11), ('Gente', 6)]\n",
      "[('PRICE', 1518), ('PRICES', 1238), ('FORECAST', 1113), ('FOR', 1044), ('PAY', 965)]\n",
      "[('Masood', 452), ('Grenfell', 451), ('Ord', 434), ('Victorian', 272), ('Rupert', 270)]\n",
      "[('JonBenet', 16)]\n",
      "[('innings', 1790), ('wickets', 937), ('overs', 884), ('Mubarak', 785), ('Wimbledon', 662)]\n",
      "[('He', 27019), ('York', 10679), ('House', 5755), ('Stock', 4123), ('Page', 3822)]\n",
      "[('the', 1244626), ('in', 526190), ('on', 294388), ('for', 233096), ('The', 208167)]\n",
      "[('PAYING', 322), ('OFF', 215), ('DOLLAR', 162), ('MONEY', 129), ('PREMIUM', 94)]\n",
      "[('told', 31128), ('seen', 7933), ('asked', 5647), ('interview', 4004), ('know', 3882)]\n",
      "[('affect', 1357), ('affecting', 445), ('undermine', 393), ('threatens', 301), ('undermined', 234)]\n",
      "[('Indian', 3811), ('rupees', 3696), ('Bombay', 1731), ('Sen', 1616), ('Sri', 1490)]\n",
      "[('does', 7716), ('want', 6892), ('wanted', 3893), ('wants', 3495), ('me', 3164)]\n",
      "[('Australian', 6770), ('Sydney', 3113), ('Melbourne', 881), ('Queensland', 848), ('NZ', 630)]\n",
      "[('Coast', 1487), ('wing', 773), ('Command', 193), ('Wing', 97), ('Room', 91)]\n",
      "[('five', 13722), ('minimum', 1793), ('jail', 1611), ('maximum', 1323), ('sentence', 1097)]\n",
      "[('salmonella', 28), ('cadmium', 23), ('E.coli', 14), ('rabies', 12), ('melamine', 7)]\n",
      "[('Tung', 707), ('Chang', 520), ('Chan', 506), ('Seng', 499), ('Wang', 397)]\n",
      "[('no', 26582), ('there', 22979), ('any', 17640), ('may', 12666), ('There', 10349)]\n",
      "[('disorder', 120), ('disability', 84), ('disorders', 75), ('deaf', 70), (\"Parkinson's\", 53)]\n",
      "[('steelmaker', 149), ('IPCL', 85), ('demerger', 75), ('Ambuja', 52), ('IFCI', 48)]\n",
      "Clusters mean length: 1242\n",
      "Clusters min length: 1\n",
      "Clusters max length: 27208\n"
     ]
    }
   ],
   "source": [
    "def print_clusters(kmeans, vocab, topn=5): \n",
    "    vocab_array = np.array(list(vocab))\n",
    "    clusters_len = np.empty((kmeans.n_clusters), dtype=int)\n",
    "    for (i, center) in enumerate(kmeans.cluster_centers_):\n",
    "        cluster_vocab = vocab_array[np.where(kmeans.labels_ == i)[0]]\n",
    "        clusters_len[i] = len(cluster_vocab)\n",
    "        cluster_counter = Counter({w: vocab[w] for w in cluster_vocab})\n",
    "#         for v in cluster_vocab:\n",
    "#             cluster_counter[v] = vocab[v]\n",
    "        print(cluster_counter.most_common(topn))\n",
    "    print(\"Clusters mean length: %d\" % (np.mean(clusters_len)))\n",
    "    print(\"Clusters min length: %d\" % (np.min(clusters_len)))\n",
    "    print(\"Clusters max length: %d\" % (np.max(clusters_len)))\n",
    "    \n",
    "\n",
    "print_clusters(kmeans, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a bow model \n",
    "\n",
    "Using the clusters from the kmeans classifier build a bow for each document. This bag of words can be normalized usign the frequency of each word with ```useFrequency=True```. \n",
    "\n",
    "How? \n",
    "* For each document\n",
    "    * For each word\n",
    "        * Obtain the w2v vector for that word\n",
    "        * Obtain the cluster for that vector\n",
    "        * Add 1 to that cluster in the document bow\n",
    "\n",
    "To improve the performance the cluster for each word is saved in a dictionary. This way for each word we first check that dictionary instead of first the w2v model and then the classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_bow(docs_to_bow, kmeans, w2v, reference_bows=None, useFrequency=True, verbose=False):\n",
    "    n_docs = len(docs_to_bow)\n",
    "    n_clusters = kmeans.n_clusters\n",
    "    bo_clusters = np.zeros((n_docs, n_clusters))\n",
    "\n",
    "    hashed_clusters = {}\n",
    "    for (i, doc) in enumerate(docs_to_bow):\n",
    "        for word, count in doc[\"counter\"].items():\n",
    "            if word in hashed_clusters:\n",
    "                cluster = hashed_clusters[word]\n",
    "            else:\n",
    "                cluster = kmeans.predict([w2v[word]])[0]\n",
    "                hashed_clusters[word] = cluster\n",
    "            bo_clusters[i][cluster] += count\n",
    "     \n",
    "    \n",
    "    if reference_bows is None:\n",
    "        normalized = np.round(bo_clusters / (bo_clusters + 0.001))    \n",
    "    else:\n",
    "        normalized = np.round(reference_bows / (reference_bows + 0.001))\n",
    "        n_docs = len(reference_bows)\n",
    "            \n",
    "    if useFrequency:\n",
    "        max_doc = np.max(bo_clusters, axis=1)\n",
    "        tf = bo_clusters / max_doc.reshape((bo_clusters.shape[0], 1)).repeat(bo_clusters.shape[1], axis=1)\n",
    "#         print(\"tf\")\n",
    "#         print(tf)\n",
    "        count = np.sum(normalized, axis=0) + 1\n",
    "#         print(\"count\")\n",
    "#         print(count)\n",
    "        idf = np.log(n_docs / count)\n",
    "#         print(\"idf\")\n",
    "#         print(idf)\n",
    "        boc = tf * idf\n",
    "    else:\n",
    "        boc = normalized\n",
    "    \n",
    "    if verbose:\n",
    "        for i in range(10):\n",
    "            print('Document %d: %s (sum = %.2f)' % (i + 1, np.array2string(boc[i, :]), np.sum(boc[i, :])))\n",
    "        \n",
    "    return boc\n",
    "\n",
    "# bows = clusters_bow(docs, kmeans, w2v)\n",
    "# print(bows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso model\n",
    "\n",
    "Once we have our bags of words we can study how good are our clusters. To do so we use a Lasso function feeding it with the bows. \n",
    "\n",
    "#### Build and train the model\n",
    "\n",
    "We build a Lasso model using the sklearn functions. Lasso is configured to only use positive coefficients (because it is easir to visualize them). \n",
    "\n",
    "If we do not have an ```alpha``` value the function uses cross validation to obtain it. \n",
    "\n",
    "#### Keep, split or discard clusters\n",
    "\n",
    "Using the lasso coefficients we can decide how good is a particular cluster. There are different ways to make this decision: \n",
    "\n",
    " - With ```use_mean=False``` and no value for ```deactivate_threshold``` we remove clusters with a coefficient lower than ```deactivate_value``` (typically near 0), split those with a coefficient lower than ```split_threshold``` times the maximum coefficient, and kept those with the higher coefficients. \n",
    " \n",
    "- With ```use_mean=False``` and a value for ```deactivate_threshold``` we remove clusters with a coefficient lower than ```deactivate_threshold``` times the maximum coefficient, split those with a coefficient lower than ```split_threshold``` times the maximum coefficient, and kept those with the higher coefficients. \n",
    "\n",
    "- With ```use_mean=True``` we remove clusters with a coefficient lower than the mean coefficient minus the standard deviation, split those between that value and mean plus standard coefficient, and kept those with a coefficient higher than this last value (mean + std)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lasso(bows, labels, alpha=None, verbose=False):\n",
    "    if alpha:\n",
    "        clf = linear_model.Lasso(alpha=alpha, positive=True)\n",
    "    else:\n",
    "        clf = linear_model.LassoCV(positive=True)\n",
    "    clf.fit(bows, labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Lasso coefficients: %s' % (np.array2string(clf.coef_, suppress_small=True)))\n",
    "        \n",
    "    if alpha:\n",
    "        return clf, clf.alpha_\n",
    "    else:\n",
    "        return clf, alpha\n",
    "\n",
    "# lasso = fit_lasso(bows, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_lasso(lasso_values, deactivate_value=0.1, deactivate_threshold=None, \n",
    "                split_threshold=0.6, use_mean=False, verbose=False):  \n",
    "    \n",
    "    if use_mean:\n",
    "        mean = np.mean([i for i in np.abs(lasso_values) if i > 0])\n",
    "        std = np.std([i for i in np.abs(lasso_values) if i > 0])\n",
    "        deactivate_value = mean - std\n",
    "        split_value = mean + std\n",
    "    else:\n",
    "        max_w = np.max(np.abs(lasso_values))\n",
    "        split_value = split_threshold * max_w\n",
    "        if deactivate_threshold:\n",
    "            deactivate_value = deactivate_threshold * max_w\n",
    "        \n",
    "    if verbose:\n",
    "        print('Deactivate value: %.2f' % (deactivate_value))\n",
    "        print('Split value: %.2f' % (split_value))\n",
    "    \n",
    "    deactivate = [x <= deactivate_value for x in np.abs(lasso_values)]\n",
    "    split = [x > deactivate_value and x <= split_value for x in np.abs(lasso_values)]\n",
    "    keep = [x > split_value for x in np.abs(lasso_values)]\n",
    "    \n",
    "    return keep, split, deactivate\n",
    "\n",
    "# keep, split = study_lasso(lasso.coef_)\n",
    "\n",
    "# print('Keeping %d clusters' % (len([x for x in keep if x == True])))\n",
    "# print('Spliting %d clusters' % (len([x for x in split if x == True])))\n",
    "\n",
    "# new_count = len([x for x in keep if x == True]) + 2 * len([x for x in split if x == True])\n",
    "# print('Using %d clusters in the next iteration' % (new_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build new clusters\n",
    "\n",
    "Once we know which clusters to keep and which to split, we can build the new ones. \n",
    "\n",
    "To split one cluster we need to select all the points belonging to that cluster and classify then using 2 neighbourds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centers(kmeans, X, keep, split):\n",
    "    n_centers = len([x for x in keep if x == True]) + 2 * len([x for x in split if x == True])\n",
    "    updated_centers = np.empty((n_centers, kmeans.cluster_centers_.shape[1]))\n",
    "    new_idx = 0\n",
    "    for i in range(len(keep)):\n",
    "        if keep[i]:\n",
    "            updated_centers[new_idx, :] = kmeans.cluster_centers_[i, :]\n",
    "            new_idx += 1\n",
    "        if split[i]:\n",
    "            # create kmeans with this data\n",
    "            newX = X[np.where(kmeans.labels_ == i)[0], :]\n",
    "            if (newX.shape[0] < 2):\n",
    "                continue\n",
    "            small_class = MiniBatchKMeans(n_clusters=2, random_state=0, compute_labels=True)\n",
    "            small_class.fit(newX)\n",
    "            updated_centers[new_idx, :] = small_class.cluster_centers_[0, :]\n",
    "            updated_centers[new_idx + 1, :] = small_class.cluster_centers_[1, :]\n",
    "            new_idx += 2\n",
    "            \n",
    "    return updated_centers[:new_idx, :]\n",
    "        \n",
    "# new_centers = update_centers(kmeans, X, keep, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataset(kmeans, X, vocab, deactivate, keep_if_big=None):\n",
    "    if keep_if_big:\n",
    "        clusters = [i for i in range(len(deactivate)) if deactivate[i] and \n",
    "                len(np.where(kmeans.labels_ == i)[0]) <= keep_if_big]\n",
    "    else:\n",
    "        clusters = [i for i in range(len(deactivate)) if deactivate[i]]\n",
    "    words = np.empty((0,), dtype=np.int)\n",
    "    for cluster_i in clusters:\n",
    "        words = np.concatenate((words, np.array(np.where(kmeans.labels_ == cluster_i)[0])))\n",
    "    mask = np.ones(X.shape[0], dtype=bool)\n",
    "    mask[words] = False\n",
    "    X = X[mask, :]\n",
    "    new_vocab = {w: c for i, (w, c) in enumerate(vocab.items()) if mask[i]}\n",
    "    return X, new_vocab\n",
    "\n",
    "# X2, vocab2 = update_dataset(kmeans, X, vocab, deactivate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the classifier\n",
    "\n",
    "We have to use the new clusters to classify the data in the following steps. One possible to solution to use these new clusters is to update the classifier centers and the relevant attributes. Then we can use the classifier's function ```predict``` as before. Moreover, to keep using this classifier to build the next cluster we also need to update the ```labels_``` and ```counts_``` parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_kmeans(kmeans, new_centers, x):\n",
    "    kmeans.cluster_centers_ = new_centers\n",
    "    kmeans.n_clusters = len(new_centers)\n",
    "    kmeans.labels_, _ = kmeans._labels_inertia_minibatch(x)\n",
    "    kmeans.counts_ = np.zeros(kmeans.n_clusters, dtype=np.int32)\n",
    "    for i in range(kmeans.n_clusters):\n",
    "        kmeans.counts_[i] = np.sum(kmeans.labels_ == i)\n",
    "    \n",
    "    return kmeans\n",
    "\n",
    "# kmeans = update_kmeans(kmeans, new_centers, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(lasso, bows, y_true, threshold=None):\n",
    "    y_predicted = lasso.predict(bows)\n",
    "    print(\"Predicted:\")\n",
    "    print(y_predicted[:10])\n",
    "    print(y_predicted[-10:])\n",
    "    print(\"True:\")\n",
    "    print(y_true[:10])\n",
    "    print(y_true[-10:])\n",
    "    if threshold is None:\n",
    "        threshold = np.mean(y_predicted)\n",
    "    y_predicted = [1 if i > threshold else 0 for i in y_predicted]\n",
    "    accuracy = accuracy_score(y_val, y_predicted)\n",
    "    kappa = cohen_kappa_score(y_val, y_predicted)\n",
    "    \n",
    "    print('Accuracy: %.3f' % (accuracy))\n",
    "    print('Kappa: %.3f' % (kappa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play time\n",
    "\n",
    "Start by saving the first kmeans (so we can use it multiple times) and printing the first set of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Liberal', 874), ('Chretien', 331), ('Liberals', 168), ('Axworthy', 110), ('Mulroney', 63)]\n",
      "[('barrels', 2418), ('refinery', 1543), ('Petroleum', 1531), ('bpd', 1375), ('pipeline', 1225)]\n",
      "[('MDL', 12), ('demurrage', 11), ('McMoRan', 6)]\n",
      "[('War', 2700), ('Digital', 355), ('Pen', 236), ('Extraordinary', 234), ('Book', 233)]\n",
      "[('Securities', 6042), ('property', 3343), ('land', 3137), ('Holdings', 2313), ('Investment', 2049)]\n",
      "[('analyst', 5205), ('Research', 1822), ('Results', 1293), ('Statistics', 1074), ('Latest', 996)]\n",
      "[('Conrail', 495), ('highway', 476), ('Rt', 256), ('Highway', 114), ('Interstate', 87)]\n",
      "[('Instruments', 176), ('Controls', 58), ('sensors', 50), ('detectors', 36), ('Components', 28)]\n",
      "[('setting', 1714), ('creating', 981), ('dealing', 870), ('critics', 638), ('ignored', 483)]\n",
      "[('pesos', 2058), ('baht', 1416), ('ringgit', 1249), ('Philippine', 1237), ('rupiah', 1196)]\n",
      "[('weapons', 1940), ('guns', 428), ('artillery', 387), ('ammunition', 297), ('weapon', 247)]\n",
      "[('British', 12083), ('German', 10280), ('Russian', 6800), ('Italian', 4273), ('Yeltsin', 3203)]\n",
      "[('election', 9302), ('elections', 7213), ('vote', 6348), ('polls', 2396), ('voters', 2368)]\n",
      "[('lung', 359), ('breast', 342), ('brain', 321), ('prostate', 220), ('implants', 214)]\n",
      "[('ASEAN', 1182), ('Cambodia', 956), ('Burma', 898), ('Zimbabwe', 855), ('Sudan', 647)]\n",
      "[('forecast', 5327), ('forecasts', 2116), ('economists', 1932), ('Economists', 743), ('rainfall', 334)]\n",
      "[('earthquake', 371), ('quake', 208), ('tremor', 99), ('earthquakes', 89), ('landslides', 70)]\n",
      "[('chase', 163), ('suitors', 26), ('peloton', 10), ('pretenders', 8), ('pursuers', 7)]\n",
      "[('shekels', 703), ('Aviv', 551), ('shekel', 237), ('Yitzhak', 211), ('Likud', 188)]\n",
      "[('U.S', 39459), ('countries', 11643), ('China', 10364), ('yen', 9919), ('world', 9361)]\n",
      "[('difficult', 3043), (\"I'm\", 2565), ('different', 2312), ('bad', 2024), ('tight', 1663)]\n",
      "[('total', 8709), ('compared', 7179), ('recorded', 1184), ('accounting', 1142), ('offset', 950)]\n",
      "[('said', 330822), ('not', 84663), ('people', 23025), ('added', 16390), ('statement', 15401)]\n",
      "[('budget', 11554), ('plans', 10317), ('plan', 8967), ('announced', 8113), ('agreed', 7729)]\n",
      "[('Eurostat', 121), ('Ipsos', 31), ('Eurobarometer', 9)]\n",
      "[('Mobutu', 1782), ('businessmen', 847), ('aides', 572), ('servants', 492), ('speculators', 415)]\n",
      "[('Holyfield', 106), ('Boxing', 102), ('boxing', 98), ('WBC', 34), ('boxer', 26)]\n",
      "[('are', 78020), ('or', 38329), ('more', 37430), ('about', 36698), ('A', 29500)]\n",
      "[('percent', 92059), ('billion', 33160), ('per', 19695), ('prices', 17533), ('rate', 15168)]\n",
      "[('mass', 1356), ('Catholic', 1136), ('funeral', 336), ('pilgrims', 323), ('Mass', 252)]\n",
      "[('Diario', 39), ('Espanol', 22), ('Vive', 11), ('Gente', 6)]\n",
      "[('PRICE', 1518), ('PRICES', 1238), ('FORECAST', 1113), ('FOR', 1044), ('PAY', 965)]\n",
      "[('Masood', 452), ('Grenfell', 451), ('Ord', 434), ('Victorian', 272), ('Rupert', 270)]\n",
      "[('JonBenet', 16)]\n",
      "[('innings', 1790), ('wickets', 937), ('overs', 884), ('Mubarak', 785), ('Wimbledon', 662)]\n",
      "[('He', 27019), ('York', 10679), ('House', 5755), ('Stock', 4123), ('Page', 3822)]\n",
      "[('the', 1244626), ('in', 526190), ('on', 294388), ('for', 233096), ('The', 208167)]\n",
      "[('PAYING', 322), ('OFF', 215), ('DOLLAR', 162), ('MONEY', 129), ('PREMIUM', 94)]\n",
      "[('told', 31128), ('seen', 7933), ('asked', 5647), ('interview', 4004), ('know', 3882)]\n",
      "[('affect', 1357), ('affecting', 445), ('undermine', 393), ('threatens', 301), ('undermined', 234)]\n",
      "[('Indian', 3811), ('rupees', 3696), ('Bombay', 1731), ('Sen', 1616), ('Sri', 1490)]\n",
      "[('does', 7716), ('want', 6892), ('wanted', 3893), ('wants', 3495), ('me', 3164)]\n",
      "[('Australian', 6770), ('Sydney', 3113), ('Melbourne', 881), ('Queensland', 848), ('NZ', 630)]\n",
      "[('Coast', 1487), ('wing', 773), ('Command', 193), ('Wing', 97), ('Room', 91)]\n",
      "[('five', 13722), ('minimum', 1793), ('jail', 1611), ('maximum', 1323), ('sentence', 1097)]\n",
      "[('salmonella', 28), ('cadmium', 23), ('E.coli', 14), ('rabies', 12), ('melamine', 7)]\n",
      "[('Tung', 707), ('Chang', 520), ('Chan', 506), ('Seng', 499), ('Wang', 397)]\n",
      "[('no', 26582), ('there', 22979), ('any', 17640), ('may', 12666), ('There', 10349)]\n",
      "[('disorder', 120), ('disability', 84), ('disorders', 75), ('deaf', 70), (\"Parkinson's\", 53)]\n",
      "[('steelmaker', 149), ('IPCL', 85), ('demerger', 75), ('Ambuja', 52), ('IFCI', 48)]\n",
      "Clusters mean length: 1242\n",
      "Clusters min length: 1\n",
      "Clusters max length: 27208\n"
     ]
    }
   ],
   "source": [
    "original_kmeans = deepcopy(kmeans)\n",
    "original_x = deepcopy(X)\n",
    "original_vocab = deepcopy(vocab)\n",
    "print_clusters(kmeans, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First configuration\n",
    "\n",
    "Using the mean and standard deviation to select which clusters to keep or divide.\n",
    "\n",
    "The output for each epoch is:\n",
    "\n",
    " - Current number of clusters\n",
    " - Lasso coefficients\n",
    " - Predicted values for the validation set\n",
    " - Accuracy\n",
    " - Kappa\n",
    " - Value for deactivating clusters (those with lower coefficient)\n",
    " - Value for splitting clusters (those with lower coefficient and not deactivated)\n",
    " - Clusters to be kept\n",
    " - Clusters to be split\n",
    " - Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 20\n",
    "\n",
    "# kmeans = deepcopy(original_kmeans)\n",
    "\n",
    "# alpha = None\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     print('Epoch %d of %d' % (i + 1, epochs))\n",
    "#     print('Number of clusters: %d' % (kmeans.n_clusters))\n",
    "#     bows_train = clusters_bow(x_train, kmeans, w2v, useFrequency=True, verbose=False)\n",
    "#     lasso, alpha = fit_lasso(bows_train, y_train, alpha=alpha, verbose=True)\n",
    "    \n",
    "#     bows_val = clusters_bow(x_val, kmeans, w2v, reference_bows=bows_train, useFrequency=True, verbose=False)\n",
    "#     validate(lasso, bows_val, y_val)\n",
    "    \n",
    "#     keep, split = study_lasso(lasso.coef_, use_mean=True, verbose=True)\n",
    "\n",
    "#     print('Keeping %d clusters' % (len([x for x in keep if x == True])))\n",
    "#     print('Spliting %d clusters' % (len([x for x in split if x == True])))\n",
    "    \n",
    "#     next_centers = update_centers(kmeans, X, keep, split)\n",
    "    \n",
    "#     kmeans = update_kmeans(kmeans, next_centers, X)\n",
    "#     print_clusters(kmeans, vocab)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second configuration\n",
    "\n",
    "Deactivating clusters with a coefficient lower than 0.01 and splitting those with lower value than 0.6 times the max(abs(coefficient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 20\n",
      "Number of clusters: 50\n",
      "Lasso coefficients: [  1.18    0.      0.      0.      0.      0.      0.      0.      2.927\n",
      "   0.      7.383   1.353   4.09    1.519   1.855   0.      2.057   2.342\n",
      "   0.173   0.      3.655   0.      0.      0.      1.906   7.396   1.02\n",
      "   0.      0.      2.085   0.      0.      0.      0.      0.917   0.      0.\n",
      "   0.     14.484   1.608   0.      7.305   0.      0.349   1.876   0.\n",
      "   0.118   0.      1.278   0.   ]\n",
      "Predicted:\n",
      "[ 0.302  0.768  0.302  0.458  0.302  0.302  0.302  0.667  0.404  0.798]\n",
      "[ 0.571  0.43   0.516  0.386  0.32   0.339  0.527  0.302  0.39   0.302]\n",
      "True:\n",
      "[0, 1, 1, 1, 0, 0, 1, 1, 0, 1]\n",
      "[1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "Accuracy: 0.706\n",
      "Kappa: 0.412\n",
      "Deactivate value: 1.45\n",
      "Split value: 8.69\n",
      "Keeping 1 clusters\n",
      "Spliting 14 clusters\n",
      "Deactivating 35 clusters\n",
      "Before (62100, 300)\n",
      "After (59446, 300)\n",
      "[('its', 80655), ('state', 19427), ('under', 17366), ('Commission', 8876), ('cut', 8637)]\n",
      "[('the', 1244626), ('in', 526190), ('on', 294388), ('for', 233096), ('The', 208167)]\n",
      "[('accuracy', 3136), ('confidence', 3090), ('strength', 2554), ('presence', 1320), ('ability', 1272)]\n",
      "[('military', 8769), ('forces', 6016), ('troops', 5732), ('army', 4791), ('NATO', 3638)]\n",
      "[('election', 9302), ('elections', 7213), ('poll', 3390), ('presidential', 3148), ('polls', 2396)]\n",
      "[('tax', 13185), ('parliament', 8303), ('vote', 6348), ('Democratic', 4066), ('taxes', 3986)]\n",
      "[('surgery', 1118), ('patients', 1032), ('pneumonia', 278), ('infection', 273), ('stroke', 223)]\n",
      "[('body', 2531), ('heart', 2277), ('disease', 1671), ('cancer', 1518), ('doctors', 1374)]\n",
      "[('China', 10364), ('Hong', 9259), ('Kong', 7704), ('rebels', 5026), ('Chinese', 4920)]\n",
      "[('Netanyahu', 3166), ('Arafat', 2478), ('Chirac', 1904), ('Kabila', 1293), ('Prodi', 1259)]\n",
      "[('earthquake', 371), ('explosions', 263), ('quake', 208), ('volcano', 130), ('seismic', 103)]\n",
      "[('floods', 562), ('landslides', 70), ('mudslides', 67), ('Floods', 34), ('mudslide', 26)]\n",
      "[('bidders', 349), ('bidder', 343), ('suitor', 51), ('suitors', 26)]\n",
      "[('standings', 532), ('breakaway', 385), ('pursuit', 213), ('birdies', 182), ('Spaniard', 174)]\n",
      "[('that', 164547), ('was', 163462), ('at', 151405), ('by', 138587), ('be', 114847)]\n",
      "[('his', 57029), ('Monday', 29621), ('Tuesday', 29193), ('when', 25015), ('Friday', 20273)]\n",
      "[('Ipsos', 31), ('GfK', 6)]\n",
      "[('Eurostat', 121), ('ECOFIN', 111), ('Schengen', 67), ('HICP', 62), ('INSEE', 59)]\n",
      "[('accused', 3986), ('alleged', 2753), ('allegations', 1877), ('scandal', 1546), ('scheme', 1478)]\n",
      "[('government', 45445), ('Minister', 21548), ('President', 16554), ('foreign', 14819), ('oil', 14462)]\n",
      "[('ministry', 5024), ('summit', 4570), ('rally', 3212), ('protest', 2890), ('St', 2369)]\n",
      "[('Catholic', 1136), ('church', 898), ('Pope', 781), ('Protestant', 446), ('Bishop', 261)]\n",
      "[('expressed', 1982), ('speaking', 1428), ('spoke', 1006), ('talked', 711), ('speak', 690)]\n",
      "[('impact', 3433), ('boost', 2998), ('improve', 2609), ('amid', 2504), ('cause', 2169)]\n",
      "[('damage', 2511), ('uncertainty', 1148), ('clashes', 1124), ('winds', 969), ('confirm', 963)]\n",
      "[('is', 150826), ('will', 90024), ('U.S', 39459), ('added', 16390), ('plans', 10317)]\n",
      "[('said', 330822), ('has', 91242), ('he', 88224), ('not', 84663), ('have', 83240)]\n",
      "[('percent', 92059), ('million', 73782), ('year', 50486), ('last', 41308), ('two', 39373)]\n",
      "[('charges', 5219), ('arrested', 2836), ('charged', 2457), ('prison', 2034), ('murder', 1895)]\n",
      "Clusters mean length: 2049\n",
      "Clusters min length: 2\n",
      "Clusters max length: 13232\n",
      "Epoch 2 of 20\n",
      "Number of clusters: 29\n",
      "Lasso coefficients: [  3.153  21.051   1.213   1.478   1.196   0.949   0.338   0.383   0.359\n",
      "   0.928   0.752   1.392   0.      1.004   0.      8.639   0.      0.845\n",
      "   1.236   1.746   1.063   0.388   1.243   0.      0.944   0.     58.071\n",
      "   0.      1.156]\n",
      "Predicted:\n",
      "[ 3.197  4.607  2.98   3.842  1.094  1.501  3.363  4.081  4.39   3.728]\n",
      "[ 3.729  4.473  4.511  3.001  3.076  3.43   2.949  2.067  1.484  0.875]\n",
      "True:\n",
      "[0, 1, 1, 1, 0, 0, 1, 1, 0, 1]\n",
      "[1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "Accuracy: 0.735\n",
      "Kappa: 0.471\n",
      "Deactivate value: 5.81\n",
      "Split value: 34.84\n",
      "Keeping 1 clusters\n",
      "Spliting 2 clusters\n",
      "Deactivating 26 clusters\n",
      "Before (59446, 300)\n",
      "After (57286, 300)\n",
      "[('rose', 11552), ('ended', 9723), ('fell', 9224), ('closed', 9066), ('cut', 8637)]\n",
      "[('the', 1244626), ('in', 526190), ('said', 330822), ('on', 294388), ('for', 233096)]\n",
      "[('them', 15572), ('back', 13389), ('newsroom', 13336), ('police', 12898), ('just', 12020)]\n",
      "[('he', 88224), ('his', 57029), ('European', 23424), ('PCT', 12607), ('London', 11016)]\n",
      "[('I', 33214), ('what', 11933), ('do', 11782), ('good', 9047), ('think', 8869)]\n",
      "Clusters mean length: 11457\n",
      "Clusters min length: 1444\n",
      "Clusters max length: 27993\n",
      "Epoch 3 of 20\n",
      "Number of clusters: 5\n",
      "Lasso coefficients: [  0.      0.     10.145  38.174  10.901]\n",
      "Predicted:\n",
      "[  5.985  10.906   0.894   8.031   4.917  13.826  51.603  10.432   9.591\n",
      "   9.251]\n",
      "[  7.862   8.638  10.875   9.677  10.57   27.413  12.501  52.255  29.303\n",
      "   2.694]\n",
      "True:\n",
      "[0, 1, 1, 1, 0, 0, 1, 1, 0, 1]\n",
      "[1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "Accuracy: 0.537\n",
      "Kappa: 0.073\n",
      "Deactivate value: 3.82\n",
      "Split value: 22.90\n",
      "Keeping 1 clusters\n",
      "Spliting 2 clusters\n",
      "Deactivating 2 clusters\n",
      "Before (57286, 300)\n",
      "After (57286, 300)\n",
      "[('per', 19695), ('rate', 15168), ('oil', 14462), ('growth', 12595), ('tonnes', 11785)]\n",
      "[('in', 526190), ('from', 123202), ('be', 114847), ('percent', 92059), ('were', 79616)]\n",
      "[('the', 1244626), ('on', 294388), ('for', 233096), ('The', 208167), ('was', 163462)]\n",
      "[('interest', 13559), ('peace', 9547), ('rights', 6544), ('stories', 6537), ('life', 4204)]\n",
      "[('said', 330822), ('that', 164547), ('is', 150826), ('it', 105010), ('as', 104041)]\n",
      "Clusters mean length: 11457\n",
      "Clusters min length: 1651\n",
      "Clusters max length: 30781\n",
      "Epoch 4 of 20\n",
      "Number of clusters: 5\n",
      "Lasso coefficients: [  0.     35.192   0.      7.602  13.798]\n",
      "Predicted:\n",
      "[ 2.483  2.613  2.517  2.404  1.343  1.67   0.512  2.971  1.608  3.021]\n",
      "[ 2.612  3.34   2.821  2.42   1.287  2.464  1.622  0.553  0.85   1.758]\n",
      "True:\n",
      "[0, 1, 1, 1, 0, 0, 1, 1, 0, 1]\n",
      "[1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "Accuracy: 0.658\n",
      "Kappa: 0.317\n",
      "Deactivate value: 3.52\n",
      "Split value: 21.12\n",
      "Keeping 1 clusters\n",
      "Spliting 2 clusters\n",
      "Deactivating 2 clusters\n",
      "Before (57286, 300)\n",
      "After (57286, 300)\n",
      "[('from', 123202), ('were', 79616), ('up', 50866), ('government', 45445), ('market', 41966)]\n",
      "[('Minister', 21548), ('political', 12465), ('him', 11293), ('her', 10506), ('minister', 10086)]\n",
      "[('President', 16554), ('statement', 15401), ('meeting', 14936), ('economic', 14285), ('interest', 13559)]\n",
      "[('in', 526190), ('The', 208167), ('at', 151405), ('percent', 92059), ('has', 91242)]\n",
      "[('the', 1244626), ('said', 330822), ('on', 294388), ('for', 233096), ('that', 164547)]\n",
      "Clusters mean length: 11457\n",
      "Clusters min length: 2154\n",
      "Clusters max length: 30213\n",
      "Epoch 5 of 20\n",
      "Number of clusters: 5\n",
      "Lasso coefficients: [ 0.     6.137  1.172  0.     0.   ]\n",
      "Predicted:\n",
      "[ 0.312  1.332  0.371  0.978  0.327  0.326  0.312  1.267  0.369  0.791]\n",
      "[ 0.41   0.904  0.472  0.363  0.332  0.376  0.389  0.467  0.318  0.357]\n",
      "True:\n",
      "[0, 1, 1, 1, 0, 0, 1, 1, 0, 1]\n",
      "[1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "Accuracy: 0.765\n",
      "Kappa: 0.530\n",
      "Deactivate value: 0.61\n",
      "Split value: 3.68\n",
      "Keeping 1 clusters\n",
      "Spliting 1 clusters\n",
      "Deactivating 3 clusters\n",
      "Before (57286, 300)\n",
      "After (57286, 300)\n",
      "[('he', 88224), ('his', 57029), ('who', 44986), ('I', 33214), ('when', 25015)]\n",
      "[('million', 73782), ('market', 41966), ('billion', 33160), ('company', 30511), ('shares', 22284)]\n",
      "[('the', 1244626), ('in', 526190), ('said', 330822), ('on', 294388), ('for', 233096)]\n",
      "Clusters mean length: 19095\n",
      "Clusters min length: 13846\n",
      "Clusters max length: 29300\n",
      "Epoch 6 of 20\n",
      "Number of clusters: 3\n",
      "Lasso coefficients: [ 24.689   0.      0.   ]\n",
      "Predicted:\n",
      "[ 0.387  1.088  0.483  0.781  0.234  0.443  0.516  0.684  0.299  0.749]\n",
      "[ 0.498  1.098  0.482  0.427  0.282  0.74   0.339  0.339  0.3    0.234]\n",
      "True:\n",
      "[0, 1, 1, 1, 0, 0, 1, 1, 0, 1]\n",
      "[1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "Accuracy: 0.753\n",
      "Kappa: 0.507\n",
      "Deactivate value: 2.47\n",
      "Split value: 14.81\n",
      "Keeping 1 clusters\n",
      "Spliting 0 clusters\n",
      "Deactivating 2 clusters\n",
      "Before (57286, 300)\n",
      "After (57286, 300)\n",
      "[('the', 1244626), ('in', 526190), ('said', 330822), ('on', 294388), ('for', 233096)]\n",
      "Clusters mean length: 57286\n",
      "Clusters min length: 57286\n",
      "Clusters max length: 57286\n",
      "Epoch 7 of 20\n",
      "Number of clusters: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso coefficients: [ 0.]\n",
      "Predicted:\n",
      "[ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "[ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "True:\n",
      "[0, 1, 1, 1, 0, 0, 1, 1, 0, 1]\n",
      "[1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "Accuracy: 0.500\n",
      "Kappa: 0.000\n",
      "Deactivate value: 0.00\n",
      "Split value: 0.00\n",
      "Keeping 0 clusters\n",
      "Spliting 0 clusters\n",
      "Deactivating 1 clusters\n",
      "Before (57286, 300)\n",
      "After (57286, 300)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 300)) while a minimum of 1 is required by check_pairwise_arrays.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ae45b2473110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#     kmeans = update_kmeans(kmeans, next_centers, X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprint_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-daeee3b35344>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, new_centers, X)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_centers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_centers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_labels_inertia_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/words/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_labels_inertia_minibatch\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m         results = [_labels_inertia(X[s], x_squared_norms[s],\n\u001b[0;32m-> 1502\u001b[0;31m                                    self.cluster_centers_) for s in slices]\n\u001b[0m\u001b[1;32m   1503\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minertia\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/words/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m         results = [_labels_inertia(X[s], x_squared_norms[s],\n\u001b[0;32m-> 1502\u001b[0;31m                                    self.cluster_centers_) for s in slices]\n\u001b[0m\u001b[1;32m   1503\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minertia\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/words/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_labels_inertia\u001b[0;34m(X, x_squared_norms, centers, precompute_distances, distances)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             return _labels_inertia_precompute_dense(X, x_squared_norms,\n\u001b[0;32m--> 620\u001b[0;31m                                                     centers, distances)\n\u001b[0m\u001b[1;32m    621\u001b[0m         inertia = _k_means._assign_labels_array(\n\u001b[1;32m    622\u001b[0m             X, x_squared_norms, centers, labels, distances=distances)\n",
      "\u001b[0;32m~/.anaconda3/envs/words/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_labels_inertia_precompute_dense\u001b[0;34m(X, x_squared_norms, centers, distances)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;31m# TODO: Once PR #7383 is merged use check_inputs=False in metric_kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     labels, mindist = pairwise_distances_argmin_min(\n\u001b[0;32m--> 563\u001b[0;31m         X=X, Y=centers, metric='euclidean', metric_kwargs={'squared': True})\n\u001b[0m\u001b[1;32m    564\u001b[0m     \u001b[0;31m# cython k-means code assumes int32 inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/words/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_argmin_min\u001b[0;34m(X, Y, axis, metric, batch_size, metric_kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'metric' must be a string or a callable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmetric_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/words/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n\u001b[1;32m    111\u001b[0m         Y = check_array(Y, accept_sparse='csr', dtype=dtype,\n\u001b[0;32m--> 112\u001b[0;31m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprecomputed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/words/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    460\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 462\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 300)) while a minimum of 1 is required by check_pairwise_arrays."
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "kmeans = deepcopy(original_kmeans)\n",
    "X = deepcopy(original_x)\n",
    "vocab = deepcopy(original_vocab)\n",
    "\n",
    "alpha = None\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('Epoch %d of %d' % (i + 1, epochs))\n",
    "    print('Number of clusters: %d' % (kmeans.n_clusters))\n",
    "    bows_train = clusters_bow(x_train, kmeans, w2v, useFrequency=True, verbose=False)\n",
    "    lasso, alpha = fit_lasso(bows_train, y_train, alpha=alpha, verbose=True)\n",
    "    \n",
    "    bows_val = clusters_bow(x_val, kmeans, w2v, reference_bows=bows_train, useFrequency=True, verbose=False)\n",
    "    validate(lasso, bows_val, y_val)\n",
    "    \n",
    "    keep, split, deactivate = study_lasso(lasso.coef_, deactivate_value=0.01, verbose=True)\n",
    "\n",
    "    print('Keeping %d clusters' % (len([x for x in keep if x == True])))\n",
    "    print('Spliting %d clusters' % (len([x for x in split if x == True])))\n",
    "    print('Deactivating %d clusters' % (len([x for x in deactivate if x == True])))\n",
    "    \n",
    "    next_centers = update_centers(kmeans, X, keep, split)\n",
    "    print(\"Before\", X.shape)\n",
    "    X, vocab = update_dataset(kmeans, X, vocab, deactivate, keep_if_big=1000)\n",
    "    print(\"After\", X.shape)\n",
    "    \n",
    "#     kmeans = update_kmeans(kmeans, next_centers, X)\n",
    "    kmeans.update(next_centers, X)\n",
    "    print_clusters(kmeans, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third configuration\n",
    "\n",
    "Deactivating clusters with a coefficient lower than 0.2 times the max(abs(coefficient)) and splitting those with lower value than 0.6 times the max(abs(coefficient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 20\n",
    "\n",
    "# kmeans = deepcopy(original_kmeans)\n",
    "# X = deepcopy(original_x)\n",
    "# vocab = deepcopy(original_vocab)\n",
    "\n",
    "# alpha = None \n",
    "\n",
    "# for i in range(epochs):\n",
    "#     print('Epoch %d of %d' % (i + 1, epochs))\n",
    "#     print('Number of clusters: %d' % (kmeans.n_clusters))\n",
    "#     bows_train = clusters_bow(x_train, kmeans, w2v, useFrequency=True, verbose=False)\n",
    "#     lasso, alpha = fit_lasso(bows_train, y_train, alpha=alpha, verbose=True)\n",
    "    \n",
    "#     bows_val = clusters_bow(x_val, kmeans, w2v, reference_bows=bows_train, useFrequency=True, verbose=False)\n",
    "#     validate(lasso, bows_val, y_val)\n",
    "    \n",
    "#     keep, split, deactivate = study_lasso(lasso.coef_, deactivate_threshold=0.1, verbose=True)\n",
    "\n",
    "#     print('Keeping %d clusters' % (len([x for x in keep if x == True])))\n",
    "#     print('Spliting %d clusters' % (len([x for x in split if x == True])))\n",
    "#     print('Deactivating %d clusters' % (len([x for x in deactivate if x == True])))\n",
    "    \n",
    "#     next_centers = update_centers(kmeans, X, keep, split)\n",
    "#     print(\"Before\", X.shape)\n",
    "#     X = update_dataset(kmeans, X, deactivate)\n",
    "#     print(\"After\", X.shape)    \n",
    "#     next_centers = update_centers(kmeans, X, keep, split)\n",
    "    \n",
    "#     kmeans = update_kmeans(kmeans, next_centers, X)\n",
    "#     print_clusters(kmeans, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 20\n",
    "\n",
    "# kmeans = deepcopy(original_kmeans)\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     print('Epoch %d of %d' % (i + 1, epochs))\n",
    "#     print('Number of clusters: %d' % (kmeans.n_clusters))\n",
    "#     bows_train = clusters_bow(x_train, kmeans, w2v, useFrequency=True, verbose=False)\n",
    "#     lasso = fit_lasso(bows_train, y_train, verbose=True)\n",
    "    \n",
    "#     bows_val = clusters_bow(x_val, kmeans, w2v, verbose=False)\n",
    "#     validate(lasso, bows_val, y_val, threshold=0.02)\n",
    "    \n",
    "#     keep, split = study_lasso(lasso.coef_, deactivate_value=0.01, use_mean=True, verbose=True)\n",
    "\n",
    "#     print('Keeping %d clusters' % (len([x for x in keep if x == True])))\n",
    "#     print('Spliting %d clusters' % (len([x for x in split if x == True])))\n",
    "    \n",
    "#     next_centers = update_centers(kmeans, X, keep, split)\n",
    "    \n",
    "#     kmeans = update_kmeans(kmeans, next_centers, X)\n",
    "#     print_clusters(kmeans, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 20\n",
    "\n",
    "# kmeans = deepcopy(original_kmeans)\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     print('Epoch %d of %d' % (i + 1, epochs))\n",
    "#     print('Number of clusters: %d' % (kmeans.n_clusters))\n",
    "#     bows_train = clusters_bow(x_train, kmeans, w2v, useFrequency=True, verbose=False)\n",
    "#     lasso = fit_lasso(bows_train, y_train, verbose=True)\n",
    "    \n",
    "#     bows_val = clusters_bow(x_val, kmeans, w2v, verbose=False)\n",
    "#     validate(lasso, bows_val, y_val, threshold=0.02)\n",
    "    \n",
    "#     keep, split = study_lasso(lasso.coef_, deactivate_value=0.01, verbose=True)\n",
    "\n",
    "#     print('Keeping %d clusters' % (len([x for x in keep if x == True])))\n",
    "#     print('Spliting %d clusters' % (len([x for x in split if x == True])))\n",
    "    \n",
    "#     next_centers = update_centers(kmeans, X, keep, split)\n",
    "    \n",
    "#     kmeans = update_kmeans(kmeans, next_centers, X)\n",
    "#     print_clusters(kmeans, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 20\n",
    "\n",
    "# kmeans = deepcopy(original_kmeans)\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     print('Epoch %d of %d' % (i + 1, epochs))\n",
    "#     print('Number of clusters: %d' % (kmeans.n_clusters))\n",
    "#     bows_train = clusters_bow(x_train, kmeans, w2v, useFrequency=True, verbose=False)\n",
    "#     lasso = fit_lasso(bows_train, y_train, verbose=True)\n",
    "    \n",
    "#     bows_val = clusters_bow(x_val, kmeans, w2v, verbose=False)\n",
    "#     validate(lasso, bows_val, y_val, threshold=0.02)\n",
    "    \n",
    "#     keep, split = study_lasso(lasso.coef_, deactivate_threshold=0.2, verbose=True)\n",
    "\n",
    "#     print('Keeping %d clusters' % (len([x for x in keep if x == True])))\n",
    "#     print('Spliting %d clusters' % (len([x for x in split if x == True])))\n",
    "    \n",
    "#     next_centers = update_centers(kmeans, X, keep, split)\n",
    "    \n",
    "#     kmeans = update_kmeans(kmeans, next_centers, X)\n",
    "#     print_clusters(kmeans, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
