{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleccionando palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import reuters_reader\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.externals import joblib\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from copy import deepcopy\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_selection import chi2\n",
    "from pandas import DataFrame\n",
    "import sklearn \n",
    "from scipy.sparse import csr_matrix\n",
    "# from scipy.sparse import *\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCV1 Dataset\n",
    "\n",
    "Use the function ```reuters_reader.reader(path)``` to retrieve the available documents from the rcv1 dataset stored in `path`. This function returns a generator (```reader```) which yields a single document each time we call ```next(reader)```. Each document is a dictionary with the followitn useful keys:\n",
    " - \"title\" is the title of the document\n",
    " - \"text\" is the body of the document\n",
    " - \"bip:topics:1.0\" is the list of topics\n",
    " \n",
    "There are a total of 804420 available documents, although some may have no topic.\n",
    "\n",
    "#### Building the dataset\n",
    "We build a balanced dataset that contains ```n_docs```. To get a balanced dataset we iterate through the documents generator until we have ```n_docs / 2``` documents with the desired topic and the same amount without it. \n",
    "\n",
    "#### Get the labels\n",
    "Select a topic we want to classify using the variable topic (there is a list of the topics https://gist.github.com/gavinmh/6253739 ). Then build the list of labels using a 1 for those documents with that topic and 0 otherwise\n",
    "\n",
    "#### Training and validation set\n",
    "Finally, we split the dataset using the ```train_split``` value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19960917 / 59589newsML.xml failed to parse XML.\n",
      "19970725 / 756041newsML.xml failed to parse XML.\n",
      "50000 docs with topic GCAT (from 100000)\n",
      "Training with 80000 docs\n",
      "Validating with 20000 docs\n"
     ]
    }
   ],
   "source": [
    "path = 'rcv1'\n",
    "n_docs = 100000\n",
    "train_split = 0.8\n",
    "topic = 'GCAT'\n",
    "\n",
    "docs = []\n",
    "reader = reuters_reader.reader(path)\n",
    "\n",
    "topic_true = 0\n",
    "topic_false = 0\n",
    "\n",
    "while len(docs) < n_docs:\n",
    "    doc = next(reader)\n",
    "    if doc['text'] == '':\n",
    "        continue\n",
    "    if topic in doc['bip:topics:1.0']:\n",
    "        topic_true += 1\n",
    "        if topic_true <= n_docs // 2:\n",
    "            docs.append(doc)\n",
    "    else:\n",
    "        topic_false += 1\n",
    "        if topic_false <= n_docs // 2:\n",
    "            docs.append(doc)\n",
    "     \n",
    "random.shuffle(docs)\n",
    "\n",
    "labels = np.zeros((n_docs), dtype=np.int16)\n",
    "labels = [1 if topic in doc['bip:topics:1.0'] else 0 for doc in docs]\n",
    "\n",
    "print('{} docs with topic {} (from {})'.format(np.sum(labels), topic, n_docs))\n",
    "\n",
    "split_point = int(n_docs * train_split)\n",
    "x_train, y_train = docs[:split_point], labels[:split_point]\n",
    "x_val, y_val = docs[split_point:], labels[split_point:]\n",
    "\n",
    "print('Training with {} docs'.format(len(x_train)))\n",
    "print('Validating with {} docs'.format(len(x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec model\n",
    "\n",
    "We are loading the well known word2vec model from __[Google](https://code.google.com/archive/p/word2vec/)__ which is stored in the binary file `GoogleNews-vectors-negative300.bin`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_name = 'GoogleNews-vectors-negative300.bin'\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_name, binary=True)\n",
    "# w2v_name = 'wiki.en/wiki.en.w2v.txt'\n",
    "# w2v = KeyedVectors.load_word2vec_format(w2v_name, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our vocabulary\n",
    "\n",
    "Get all the the vectors from the word2vec for our vocabulary. Our vocabulary can include all the words used in the word2vec model or be limited to the words in our dataset.\n",
    "\n",
    "We can change this behaviour with the flag ```dataset_vocabulary```. ```False``` will use all the words from the word2vec model and ```True``` will limit them to just the words that are in our dataset and in the model at the same time.\n",
    "\n",
    "There is a ```count_threshold``` to remove those words appearing very few times because they are probably errors.\n",
    "\n",
    "As we have to split each document in individual words, we already save this inside each document with the key \"counter\".\n",
    "\n",
    "After this cell, ```X``` is a matrix including all the vectors we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 61651\n"
     ]
    }
   ],
   "source": [
    "words_list = []\n",
    "words_embedding = []\n",
    "words_count = []\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "word2cluster = {}\n",
    "cluster2words = {}\n",
    "\n",
    "count_threshold = 5\n",
    "\n",
    "for doc in docs:\n",
    "    doc[\"counter\"] = Counter()\n",
    "    doc[\"word_count\"] = 0\n",
    "    words = doc[\"text\"].split()\n",
    "    words = [word.strip(string.punctuation) for word in words]\n",
    "    for word in words:\n",
    "        if word in w2v:\n",
    "            doc[\"counter\"][word] += 1\n",
    "            doc[\"word_count\"] += 1\n",
    "    for word, count in doc[\"counter\"].items():\n",
    "        try:\n",
    "            words_count[word2idx[word]] += count\n",
    "        except:\n",
    "            words_list.append(word)\n",
    "            words_count.append(count)\n",
    "            word2idx[word] = len(words_list) - 1\n",
    "                \n",
    "keep_it = [count > count_threshold for count in words_count] \n",
    "    \n",
    "words_list = [word for idx, word in enumerate(words_list) if keep_it[idx]]\n",
    "words_count = [count for idx, count in enumerate(words_count) if keep_it[idx]]\n",
    "word2idx = {word: idx for idx, word in enumerate(words_list)}\n",
    "    \n",
    "words_embedding = np.zeros((len(words_list), w2v.vector_size), dtype=np.float32)\n",
    "for idx, word in enumerate(words_list):\n",
    "    words_embedding[idx, :] += w2v[word]\n",
    "    \n",
    "idx2word = dict(enumerate(words_list))\n",
    "\n",
    "print(\"Vocabulary length: {}\".format(len(words_list)))\n",
    "\n",
    "for _ in range(100):\n",
    "    idx = random.randint(0, len(words_list) - 1)\n",
    "    word = words_list[idx]\n",
    "    assert(word2idx[word] == idx)\n",
    "    assert((w2v[word] == words_embedding[idx]).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "Represent each document using a bag-of-words model. This representation is done using term frequencyâ€“inverse document frequency (tf-idf) and stored in a sparse matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indptr = [0]\n",
    "indices = []\n",
    "data = []\n",
    "\n",
    "for doc in docs:\n",
    "    for word in doc[\"counter\"]:\n",
    "        if word in word2idx:\n",
    "            indices.append(word2idx[word])\n",
    "            data.append(1)\n",
    "    indptr.append(len(indices))\n",
    "    \n",
    "matrix = csr_matrix((data, indices, indptr), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Using the bag-of-words representation we can already classify the documents, getting a good baseline to compare our algorithms to. \n",
    "\n",
    "Note: this classification takes quite some time (a couple of hours at least). That is the reason they are commented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_point = int(n_docs * train_split)\n",
    "# baseline_x_train, baseline_y_train = matrix[:split_point], labels[:split_point]\n",
    "# baseline_x_val, baseline_y_val = matrix[split_point:], labels[split_point:]\n",
    "\n",
    "# lasso = fit_lasso(baseline_x_train, baseline_y_train)\n",
    "# validate(lasso, baseline_x_val, baseline_y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Cluster\n",
    "\n",
    "Train the first Kmeans cluster using the complete set of words in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial centers\n",
    "\n",
    "Supposedly, we can improve our results if the first time we build our clusters we know which are the good center candidates.\n",
    "\n",
    "To select this initial centers we use a feature selection algorithm and retrieve the best ranked features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['must', 'union', 'Newsroom', 'took', 'men', 'could', 'vote', 'trader', 'which', 'profit', 'workers', 'stocks', 'traded', 'force', 'time', 'Sales', 'national', 'won', 'under', 'European', 'first', 'team', 'parties', 'cents', 'percent', 'found', 'with', 'yen', 'newsroom', 'left', 'as', 'died', 'between', 'Co', 'city', 'what', 'dealers', 'work', 'Democratic', 'would', 'But', 'company', 'visit', 'where', 'campaign', 'army', 'reporters', 'win', 'her', 'since', 'stock', 'A', 'futures', 'be', 'price', 'only', 'ruling', 'trading', 'town', 'troops', 'law', 'accused', 'index', 'press', 'spokesman', 'verified', 'saying', 'home', 'Union', 'vouch', 'United', 'rose', 'I', 'accuracy', 'say', 'are', 'does', 'election', 'Ltd', 'but', 'prices', 'forces', 'elections', 'by', 'if', 'all', 'into', 'NOTE', 'years', 'no', 'called', 'over', 'traders', 'war', 'talks', 'court', 'says', 'Inc', 'parliament', 'security', 'after', 'had', 'members', 'peace', 'Saturday', 'an', 'country', 'stories', 'state', 'that', 'against', 'minister', 'leaders', 'killed', 'two', 'military', 'they', 'Party', 'He', 'opposition', 'out', 'officials', 'them', 'party', 'been', 'when', 'Sunday', 'share', 'per', 'Prime', 'has', 'police', 'political', 'Net', 'former', 'market', 'shares', 'leader', 'told', 'have', 'him', 'not', 'government', 'people', 'he', 'Minister', 'their', 'President', 'who', 'his']\n"
     ]
    }
   ],
   "source": [
    "scores, _ = chi2(matrix, labels)\n",
    "sorted_idx = np.argsort(scores, kind=\"mergesort\")[-n_clusters:]\n",
    "words = [words_list[idx] for idx in sorted_idx]\n",
    "initial_centers = np.array([words_embedding[idx] for idx in sorted_idx])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasi/.conda/envs/words/lib/python3.5/site-packages/ipykernel_launcher.py:8: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=100, compute_labels=True,\n",
       "        init=array([[-0.07031,  0.0918 , ..., -0.07666,  0.01697],\n",
       "       [-0.25195, -0.21484, ..., -0.18457, -0.03833],\n",
       "       ...,\n",
       "       [ 0.0791 ,  0.09668, ...,  0.07129, -0.05005],\n",
       "       [ 0.31836,  0.17676, ...,  0.05249, -0.00233]], dtype=float32),\n",
       "        init_size=None, max_iter=100, max_no_improvement=10,\n",
       "        n_clusters=150, n_init=3, random_state=0, reassignment_ratio=0.01,\n",
       "        tol=0.0, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, \n",
    "                         init=initial_centers, \n",
    "                         random_state=0, \n",
    "                         compute_labels=True)\n",
    "# kmeans = MiniBatchKMeans(n_clusters=n_clusters, \n",
    "#                          random_state=0, \n",
    "#                          compute_labels=True)\n",
    "kmeans.fit(words_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_embedding = deepcopy(original_words_embedding)\n",
    "# words_count = deepcopy(original_words_count)\n",
    "# word2idx = deepcopy(original_word2idx)\n",
    "# words_list = deepcopy(original_words_list)\n",
    "# kmeans = MiniBatchKMeans(n_clusters=50, \n",
    "#                          random_state=0, \n",
    "#                          compute_labels=True)\n",
    "# kmeans.fit(words_embedding)\n",
    "# word2cluster = {word: kmeans.predict([words_embedding[i]])[0] \n",
    "#                  for i, word in enumerate(words_list)}\n",
    "# cluster2words = [[] for i in range(kmeans.n_clusters)]\n",
    "# for (word, cluster) in word2cluster.items():\n",
    "#     cluster2words[cluster].append(word)\n",
    "# print_clusters(cluster2words, words_count, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2cluster = {word: kmeans.predict([words_embedding[i]])[0] \n",
    "                 for i, word in enumerate(words_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2words = [[] for i in range(kmeans.n_clusters)]\n",
    "for (word, cluster) in word2cluster.items():\n",
    "    cluster2words[cluster].append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print each cluster\n",
    "\n",
    "To have an idea of the clusters we are workings with we can print some of their neighbords. The option selected here is to print the most representative neighbords (those that appear more times in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(cluster2words, word_counts, word2idx, topn=5, print_cluster_words=True): \n",
    "    clusters_len = [len(cluster2words[i]) for i in range(len(cluster2words))]\n",
    "    if print_cluster_words:\n",
    "        for cluster_index in range(len(cluster2words)):\n",
    "            cluster_words = cluster2words[cluster_index]\n",
    "            cluster_counter = Counter({w: word_counts[word2idx[w]] for w in cluster_words})\n",
    "            print(cluster_counter.most_common(topn))\n",
    "    \n",
    "    print(\"Clusters mean length: %d\" % (np.mean(clusters_len)))\n",
    "    min_len = np.min(clusters_len)\n",
    "    print(\"Clusters min length: %d (%d clusters)\" % (min_len, sum(clusters_len == min_len)))\n",
    "    print(\"Clusters max length: %d\" % (np.max(clusters_len)))\n",
    "    \n",
    "\n",
    "# print_clusters(cluster2words, words_count, word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the documents using the clusters\n",
    "\n",
    "Using the clusters from the kmeans classifier build a bow for each document. This bag of words can be normalized usign the frequency of each word with ```useFrequency=True```. \n",
    "\n",
    "How? \n",
    "* For each document\n",
    "    * For each word\n",
    "        * Obtain the w2v vector for that word\n",
    "        * Obtain the cluster for that vector\n",
    "        * Add 1 to that cluster in the document bow\n",
    "\n",
    "To improve the performance the cluster for each word is saved in a dictionary. This way for each word we first check that dictionary instead of first the w2v model and then the classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_bow(docs, n_clusters, word2cluster, tfidf_reference=None, useFrequency=True, normalize=True, verbose=False):\n",
    "#     n_clusters = kmeans.n_clusters\n",
    "    \n",
    "    data = []\n",
    "    indices = []\n",
    "    indptr = [0]\n",
    "  \n",
    "    hashed_clusters = {}\n",
    "    for doc in docs:\n",
    "        doc_clusters = np.zeros((n_clusters,))\n",
    "        for (word, count) in doc[\"counter\"].items():\n",
    "            try:\n",
    "                cluster = word2cluster[word]\n",
    "                doc_clusters[cluster] += count\n",
    "            except:\n",
    "                pass\n",
    "#                 print(\"Skipping word: \" + word)\n",
    "        for i in range(n_clusters):\n",
    "            if doc_clusters[i]:\n",
    "                indices.append(i)\n",
    "                data.append(doc_clusters[i])\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    bo_clusters = csr_matrix((data, indices, indptr), dtype=float, shape=(len(docs), n_clusters))\n",
    "    \n",
    "    if tfidf_reference is None:\n",
    "        normalized = bo_clusters.copy()\n",
    "    else:\n",
    "        normalized = tfidf_reference.copy()\n",
    "    normalized.data[:] = 1\n",
    "            \n",
    "    if useFrequency:\n",
    "        max_doc = np.max(bo_clusters, axis=1).todense()\n",
    "        tf = bo_clusters\n",
    "        assert len(max_doc) == len(tf.indptr) - 1\n",
    "        for i in range(1, len(tf.indptr)):\n",
    "            tf.data[indptr[i-1]:indptr[i]] /= max_doc[i - 1, 0]\n",
    "        count = np.sum(normalized, axis=0) + 1\n",
    "        idf = np.log(normalized.shape[0] / count)\n",
    "        boc = tf\n",
    "        for i in range(len(tf.data)):\n",
    "            boc.data[i] *= idf[0, tf.indices[i]]\n",
    "    else:\n",
    "        if normalize:\n",
    "            boc = normalized\n",
    "        else:\n",
    "            assert tfidf is None\n",
    "            boc = bo_clusters\n",
    "            \n",
    "    if verbose:\n",
    "        for i in range(10):\n",
    "            print('Document %d: %s (sum = %.2f)' % (i + 1, np.array2string(boc[i, :]), np.sum(boc[i, :])))\n",
    "        \n",
    "    return boc\n",
    "\n",
    "# bows = clusters_bow(docs[:10], kmeans, word2cluster, useFrequency=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good is the cluster for this classification task?\n",
    "\n",
    "Words from this cluster are more usual in documents of this class or in documents of other classes?\n",
    "\n",
    "We can measure it obtaing the value $p_i$ for each cluster $i$. As we have a binary classification topic, documents belonging to the topic have a label 1 and 0 if not\n",
    " \n",
    "\n",
    "$$ p_i = \\frac{\\sum_{j = doc}tf(j, i) \\mid label(j) = 1}{\\sum_{j = doc}tf(j, i)} $$\n",
    "\n",
    "where $tf\\_idf(j, i)$ is the value obtained for document $j$ and cluster $i$ in the bag-of-clusters representation. Using that value we can measure how good each cluster is with:\n",
    "\n",
    "$$ Uncertainty(cluster_i) = -p_i * log_2(p_i) - (1-p_i) *log_2(1-p_i)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_uncertainty(bows, labels):\n",
    "#     scores, _ = chi2(bows, labels)\n",
    "    bows_copy = bows.copy()\n",
    "    total = bows_copy.sum(axis=0)\n",
    "    \n",
    "    negative_rows = [i for (i, l) in enumerate(labels) if not l]\n",
    "    for row in negative_rows:\n",
    "        bows_copy.data[bows.indptr[row]:bows.indptr[row + 1]] = 0\n",
    "    possitive = bows_copy.sum(axis=0)\n",
    "    p = possitive / (total + 0.00000001)\n",
    "    p = p.A1\n",
    "    # Add 0.000001 to avoid nan with log2(0)\n",
    "    uncertainty = -p * np.log2(p + 0.00000001) - (1 - p) * np.log2(1.00000001 - p)\n",
    "    \n",
    "#     return scores\n",
    "    return uncertainty\n",
    "\n",
    "# uncertainty = clusters_uncertainty(bows, y_train[:10])\n",
    "# print(uncertainty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a classifier\n",
    "\n",
    "Use the representation of the documents to fit a classifier.\n",
    "\n",
    "This classifier must return a weight for each feature (each cluster in this case) so we can decide which are the important clusters and which are not relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM classifier\n",
    "\n",
    "Linear classifier, otherwise we don't have meaningful weights for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svm(bows, labels):\n",
    "    features2c = {50: 8, 100: 8, 150: 8, 200: 8, 250: 1, 300: 0.5, 350: 0.25, 400: 0.25, \n",
    "                    500: 0.25, 750: 0.25, 1000: 0.03, 1500: 0.03, 2000: 0.015625, 5000: 0.0078125, 10000: 0.0078125}\n",
    "    n_features = bows.shape[1]\n",
    "\n",
    "    features_thresholds = sorted(list(features2c.keys()))\n",
    "    for i, val in enumerate(features_thresholds):\n",
    "        if n_features < val:\n",
    "            break\n",
    "\n",
    "    if i == 0:\n",
    "        selected_c = features2c[features_thresholds[0]]\n",
    "    elif i == len(features_thresholds) - 1:\n",
    "        selected_c = features2c[features_thresholds[len(features_thresholds) - 1]]\n",
    "    else:\n",
    "        left_threshold = features_thresholds[i - 1]\n",
    "        right_threshold = features_thresholds[i]\n",
    "        if n_features - left_threshold > right_threshold - n_features:\n",
    "            selected_c = features2c[features_thresholds[i]]\n",
    "        else:\n",
    "            selected_c = features2c[features_thresholds[i - 1]]\n",
    "\n",
    "    clf = sklearn.svm.LinearSVC(dual=False, C=selected_c)\n",
    "    clf.fit(bows, labels)\n",
    "        \n",
    "    return clf\n",
    "\n",
    "# svm_classifier = fit_svm(bows, y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso classifier\n",
    "\n",
    "We build a Lasso model using the sklearn functions. Lasso is configured to only use positive coefficients (because it is easir to visualize them).\n",
    "\n",
    "If we do not have an alpha value the function uses cross validation to obtain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lasso(bows, labels, alpha=None, verbose=False):\n",
    "    if alpha:\n",
    "        clf = linear_model.Lasso(alpha=alpha, positive=True)\n",
    "    else:\n",
    "        clf = linear_model.LassoCV(positive=True)\n",
    "    clf.fit(bows, labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Lasso coefficients: %s' % (np.array2string(clf.coef_, suppress_small=True)))\n",
    "        \n",
    "    if alpha:\n",
    "        return clf, clf.alpha_\n",
    "    else:\n",
    "        return clf, alpha\n",
    "\n",
    "# lasso, alpha = fit_lasso(bows_train, y_train)\n",
    "# lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the clusters\n",
    "\n",
    "#### Keep, discard, or split each cluster?\n",
    "\n",
    "We can study what to do with each cluster using the previous uncertainty and the weight given by the classifier.\n",
    "\n",
    "For a certain cluster, if the weight given by the classifier is low, the cluster is useless, at least for this classification problem. But, the cluster may be bad for the classification task because it contains a lot of semmantic families, some in favor some against the label. We can check this using the uncertainty value. \n",
    "\n",
    "If the cluster has a low weight and low uncertainty we can deactivate it, removing the words it contains from the complete set of words. \n",
    "If the cluster has a low weight and high uncertainty we can split it and hopefully the new 2 clusters will have less uncertainty. \n",
    "\n",
    "Currently, to consider low weight the value must be lower than 0.4 times the highest weight. \n",
    "\n",
    "We assume low uncertainty for values lower than 0.7219 (meaning that p is lower than 0.2 or higher than 0.8) and high uncertainty for values higher than 0.8813 (p between 0.3 and 0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_clusters(cluster_weights, uncertainty, cluster2words, \n",
    "                   low_uncertainty_p=0.5, high_uncertainty_p=0.5, cluster_weights_p=0.5):\n",
    "    assert len(cluster_weights) == len(uncertainty)\n",
    "        \n",
    "    clusters = range(len(cluster_weights))\n",
    "        \n",
    "    idx = int(low_uncertainty_p * len(uncertainty)) - 1 if low_uncertainty_p > 0 else 0\n",
    "    low_uncertainty_threshold = np.sort(uncertainty)[idx]\n",
    "    \n",
    "    idx = int(high_uncertainty_p * len(uncertainty)) - 1 if high_uncertainty_p > 0 else 0\n",
    "    high_uncertainty_threshold = np.sort(uncertainty)[idx]\n",
    "\n",
    "    cluster_weights = cluster_weights * cluster_weights\n",
    "    idx = int(cluster_weights_p * len(cluster_weights)) - 1 if cluster_weights_p > 0 else 0\n",
    "    bad_cluster_weigth = np.sort(cluster_weights)[idx]\n",
    "     \n",
    "    deactivate = [cluster_weights[c] <= bad_cluster_weigth \n",
    "                  and uncertainty[c] < low_uncertainty_threshold \n",
    "                  for c in clusters]\n",
    "    split = [cluster_weights[c] <= bad_cluster_weigth\n",
    "             and uncertainty[c] > high_uncertainty_threshold\n",
    "             for c in clusters]\n",
    "    \n",
    "#     for i in range(len(cluster_weights)):\n",
    "#         print(\"cluster %d: %f %f %d\" % (i, cluster_weights[i], uncertainty[i], len(cluster2words[i])))    \n",
    "    \n",
    "    keep = [not split[c] and not deactivate[c] for c in clusters]\n",
    "\n",
    "    return keep, split, deactivate\n",
    "\n",
    "# keep, split, deactivate = study_clusters(svm_classifier.coef_[0], uncertainty, 0.5, 0.5)\n",
    "\n",
    "# print('Keeping %d clusters' % (sum(keep)))\n",
    "# print('Spliting %d clusters' % (sum(split)))\n",
    "# print('Deactivating %d clusters' % (sum(deactivate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build new clusters\n",
    "\n",
    "Once we know which clusters to keep and which to split, we can build the new ones. \n",
    "\n",
    "To split one cluster we need to select all the points belonging to that cluster and use a 2-means clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_clusters(words_embedding, word2idx, word2cluster, cluster2words, keep, split):    \n",
    "    new_idx = 0\n",
    "    for idx in range(len(keep)):\n",
    "        if keep[idx]:\n",
    "            for word in cluster2words[idx]:\n",
    "                word2cluster[word] = new_idx\n",
    "            new_idx += 1\n",
    "#             print('Keeping cluster %d' % idx)\n",
    "        if split[idx]:\n",
    "            # create kmeans with this data\n",
    "            cluster_words = cluster2words[idx]\n",
    "            if len(cluster_words) < 2:\n",
    "#                 print('Cannot split cluster %d' % idx)\n",
    "                for word in cluster2words[idx]:\n",
    "                    word2cluster[word] = new_idx\n",
    "                new_idx += 1\n",
    "            else: \n",
    "#                 print('Splitting cluster %d' % idx)\n",
    "#                 embeddings_idx = [word2idx[word] for word in cluster_words if word in word2idx]\n",
    "                embeddings_idx = [word2idx[word] for word in cluster_words]\n",
    "                cluster_embeddings = words_embedding[embeddings_idx]\n",
    "                kmeans = MiniBatchKMeans(n_clusters=2, random_state=0, compute_labels=True)\n",
    "                kmeans.fit(cluster_embeddings)\n",
    "                for word, embedding in zip(cluster_words, cluster_embeddings):\n",
    "                    word2cluster[word] = new_idx + kmeans.predict([embedding])[0] \n",
    "                new_idx += 2\n",
    "            \n",
    "    return word2cluster, new_idx\n",
    "        \n",
    "def deactivate_clusters(word2idx, cluster2words, words_list, words_count, words_embedding, deactivate, split, keep):\n",
    "#     print('Deactivating %d clusters' % (sum(deactivate)))\n",
    "#     clusters_idx = [idx for idx in range(len(deactivate)) if deactivate[idx] or \n",
    "#                    (split[idx] and len(cluster2words[idx]) < 2)]\n",
    "    clusters_idx = [idx for idx in range(len(deactivate)) if deactivate[idx]]\n",
    "    words_idx = []\n",
    "    for idx in clusters_idx:\n",
    "        words_idx.append([word2idx[word] for word in cluster2words[idx]])\n",
    "    \n",
    "    if words_idx: \n",
    "        words_idx = np.concatenate(words_idx)\n",
    "\n",
    "    keep = [True for _ in range(len(words_list))]\n",
    "    for idx in words_idx:\n",
    "        keep[idx] = False\n",
    "    \n",
    "    words_list = [word for idx, word in enumerate(words_list) if keep[idx]]\n",
    "    words_count = [count for idx, count in enumerate(words_count) if keep[idx]]\n",
    "    words_embedding = words_embedding[keep]\n",
    "    \n",
    "    return words_list, words_count, words_embedding\n",
    "\n",
    "# words_list, words_count, words_embedding = update_dataset(word2idx, cluster2words, words_list, words_count, words_embedding, deactivate)\n",
    "\n",
    "# word2idx = {word: idx for idx, word in enumerate(words_list)}\n",
    "# idx2word = dict(enumerate(words_list))\n",
    "\n",
    "# word2cluster = update_centers(words_embedding, word2idx, word2cluster, cluster2words, keep, split)    \n",
    "\n",
    "# cluster2words = [[] for i in range(kmeans.n_clusters)]\n",
    "# for (word, cluster) in word2cluster.items():\n",
    "#     cluster2words[cluster].append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove words from the dataset\n",
    "\n",
    "If we have to deactivate some cluster, we need to remove the words it included from the original set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_dataset(kmeans, X, vocab, deactivate):\n",
    "#     clusters = [i for i in range(len(deactivate)) if deactivate[i]]\n",
    "#     words = np.empty((0,), dtype=np.int)\n",
    "#     for cluster_i in clusters:\n",
    "#         words = np.concatenate((words, np.array(np.where(kmeans.labels_ == cluster_i)[0])))\n",
    "#     mask = np.ones(X.shape[0], dtype=bool)\n",
    "#     mask[words] = False\n",
    "#     X = X[mask, :]\n",
    "#     new_vocab = {w: c for i, (w, c) in enumerate(vocab.items()) if mask[i]}\n",
    "#     return X, new_vocab\n",
    "\n",
    "# X2, vocab2 = update_dataset(kmeans, X, vocab, deactivate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the classifier\n",
    "\n",
    "We have to use the new clusters to classify the data in the following steps. One possible to solution to use these new clusters is to update the classifier centers and the relevant attributes. Then we can use the classifier's function ```predict``` as before. Moreover, to keep using this classifier to build the next cluster we also need to update the ```labels_``` and ```counts_``` parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_kmeans(kmeans, new_centers, x):\n",
    "#     kmeans.cluster_centers_ = new_centers\n",
    "#     kmeans.n_clusters = len(new_centers)\n",
    "#     kmeans.labels_, _ = kmeans._labels_inertia_minibatch(x)\n",
    "#     kmeans.counts_ = np.zeros(kmeans.n_clusters, dtype=np.int32)\n",
    "#     for i in range(kmeans.n_clusters):\n",
    "#         kmeans.counts_[i] = np.sum(kmeans.labels_ == i)\n",
    "#         print('Cluster %d contains %d elements' % (i, kmeans.counts_[i]))\n",
    "    \n",
    "#     return kmeans\n",
    "\n",
    "# kmeans = update_kmeans(kmeans, new_centers, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(lasso, bows, y_true, threshold=None):\n",
    "    y_predicted = lasso.predict(bows)\n",
    "#     print(\"Predicted:\")\n",
    "#     print(y_predicted[:10])\n",
    "#     print(y_predicted[-10:])\n",
    "#     print(\"True:\")\n",
    "#     print(y_true[:10])\n",
    "#     print(y_true[-10:])\n",
    "    if threshold is None:\n",
    "        threshold = np.mean(y_predicted)\n",
    "    y_predicted = [1 if i > threshold else 0 for i in y_predicted]\n",
    "    accuracy = accuracy_score(y_val, y_predicted)\n",
    "    kappa = cohen_kappa_score(y_val, y_predicted)\n",
    "    \n",
    "    return accuracy, kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play time\n",
    "\n",
    "Start by saving the first kmeans (so we can use it multiple times) and printing the first set of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_words_list = deepcopy(words_list)\n",
    "original_words_count = deepcopy(words_count)\n",
    "original_words_embedding = deepcopy(words_embedding)\n",
    "original_word2cluster = deepcopy(word2cluster)\n",
    "original_cluster2words = deepcopy(cluster2words)\n",
    "original_word2idx = deepcopy(word2idx)\n",
    "original_idx2word = deepcopy(idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for low_uncertainty_p in range(11):\n",
    "#     for high_uncertainty_p in range(low_uncertainty_p, 11):\n",
    "#         epochs = 200\n",
    "\n",
    "#         patience = 5\n",
    "#         tries = 0\n",
    "#         best_accuracy = 0\n",
    "\n",
    "#         words_list = deepcopy(original_words_list)\n",
    "#         words_count = deepcopy(original_words_count)\n",
    "#         words_embedding = deepcopy(original_words_embedding)\n",
    "#         word2cluster = deepcopy(original_word2cluster)\n",
    "#         cluster2words = deepcopy(original_cluster2words)\n",
    "#         word2idx = deepcopy(original_word2idx)\n",
    "#         idx2word = deepcopy(original_idx2word)\n",
    "#         n_clusters = 50\n",
    "\n",
    "#         history = []\n",
    "\n",
    "#         print_clusters(cluster2words, words_count, word2idx, print_cluster_words=False)\n",
    "\n",
    "#         print('Using low_uncertainty_p: %f' % (low_uncertainty_p / 10))\n",
    "#         print('Using high_uncertainty_p: %f' % (high_uncertainty_p / 10))\n",
    "        \n",
    "#         for i in range(epochs):\n",
    "#             print('Epoch %d of %d' % (i + 1, epochs))\n",
    "#             print('Number of clusters: %d' % (n_clusters))\n",
    "#             bows_train = clusters_bow(x_train, n_clusters, word2cluster, useFrequency=False)\n",
    "#             svm = fit_svm(bows_train, y_train)\n",
    "\n",
    "#             weights, _ = chi2(bows_train, y_train)\n",
    "#             uncertainty = clusters_uncertainty(bows_train, y_train)\n",
    "\n",
    "#         #     print(uncertainty)\n",
    "\n",
    "#             bows_val = clusters_bow(x_val, n_clusters, word2cluster, useFrequency=False)    \n",
    "#             accuracy, kappa = validate(svm, bows_val, y_val)\n",
    "\n",
    "#             print('Accuracy: %.3f' % (accuracy))\n",
    "\n",
    "#             if accuracy > best_accuracy:\n",
    "#                 best_accuracy = accuracy\n",
    "#         #             pickle.dump([kmeans, svm], open(\"best.classifier.pckl\", \"wb\"))\n",
    "#                 tries = 0\n",
    "#             else:\n",
    "#                 tries += 1\n",
    "#                 if tries >= patience:\n",
    "#         #                 [kmeans, svm] = pickle.load(open(\"best.classifier.pckl\", \"rb\"))\n",
    "#                     break      \n",
    "\n",
    "#             keep, split, deactivate = study_clusters(weights, uncertainty, cluster2words, low_uncertainty_p=low_uncertainty_p/10, high_uncertainty_p=high_uncertainty_p/10)\n",
    "#         #     keep, split, deactivate = study_clusters(svm.coef_[0], uncertainty, cluster2words)\n",
    "\n",
    "#             keep_count = sum(keep)\n",
    "#             split_count = sum(split)\n",
    "#             deactivate_count = sum(deactivate)\n",
    "\n",
    "\n",
    "#             before = len(words_list)\n",
    "#             assert before == len(words_count)\n",
    "#             assert before == words_embedding.shape[0]\n",
    "\n",
    "#             words_list, words_count, words_embedding = deactivate_clusters(word2idx, cluster2words, words_list, words_count, words_embedding, deactivate, split, keep)\n",
    "\n",
    "#             word2idx = {word: idx for idx, word in enumerate(words_list)}\n",
    "#             idx2word = dict(enumerate(words_list))\n",
    "\n",
    "#             word2cluster, next_n_clusters = split_clusters(words_embedding, word2idx, word2cluster, cluster2words, keep, split)    \n",
    "\n",
    "#             assert next_n_clusters <= n_clusters - deactivate_count + split_count    \n",
    "\n",
    "#             cluster2words = [[] for i in range(next_n_clusters)]\n",
    "#             for word in words_list:\n",
    "#                 cluster2words[word2cluster[word]].append(word)\n",
    "\n",
    "#             after = len(words_list)\n",
    "#             assert after == len(words_count)\n",
    "#             assert after == words_embedding.shape[0]\n",
    "\n",
    "#             print_clusters(cluster2words, words_count, word2idx, print_cluster_words=False)\n",
    "\n",
    "#             history.append([\n",
    "#                 i, \n",
    "#                 n_clusters, \n",
    "#                 keep_count, \n",
    "#                 split_count,\n",
    "#                 deactivate_count,\n",
    "#                 before,\n",
    "#                 after,\n",
    "#                 accuracy,\n",
    "#                 kappa\n",
    "#             ])\n",
    "\n",
    "#             n_clusters = next_n_clusters\n",
    "\n",
    "#         columns = [\n",
    "#         'Epoch',\n",
    "#         'Clusters', \n",
    "#         'Keep', \n",
    "#         'Split',\n",
    "#         'Deactivate',\n",
    "#         'Words before',\n",
    "#         'Words after',\n",
    "#         'Accuracy',\n",
    "#         'Kappa'\n",
    "#         ]\n",
    "\n",
    "#         df = DataFrame(history, columns=columns)\n",
    "#         display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for cluster_weights_p in range(11):\n",
    "\n",
    "#     epochs = 200\n",
    "\n",
    "#     patience = 5\n",
    "#     tries = 0\n",
    "#     best_accuracy = 0\n",
    "\n",
    "#     words_list = deepcopy(original_words_list)\n",
    "#     words_count = deepcopy(original_words_count)\n",
    "#     words_embedding = deepcopy(original_words_embedding)\n",
    "#     word2cluster = deepcopy(original_word2cluster)\n",
    "#     cluster2words = deepcopy(original_cluster2words)\n",
    "#     word2idx = deepcopy(original_word2idx)\n",
    "#     idx2word = deepcopy(original_idx2word)\n",
    "#     n_clusters = 50\n",
    "\n",
    "#     history = []\n",
    "\n",
    "#     print_clusters(cluster2words, words_count, word2idx, print_cluster_words=False)\n",
    "\n",
    "#     print('Using cluster_weights_p: %f' % (cluster_weights_p / 10))\n",
    "#     for i in range(epochs):\n",
    "#         print('Epoch %d of %d' % (i + 1, epochs))\n",
    "#         print('Number of clusters: %d' % (n_clusters))\n",
    "#         bows_train = clusters_bow(x_train, n_clusters, word2cluster, useFrequency=False)\n",
    "#         svm = fit_svm(bows_train, y_train)\n",
    "\n",
    "#         weights, _ = chi2(bows_train, y_train)\n",
    "#         uncertainty = clusters_uncertainty(bows_train, y_train)\n",
    "\n",
    "#     #     print(uncertainty)\n",
    "\n",
    "#         bows_val = clusters_bow(x_val, n_clusters, word2cluster, useFrequency=False)    \n",
    "#         accuracy, kappa = validate(svm, bows_val, y_val)\n",
    "\n",
    "#         print('Accuracy: %.3f' % (accuracy))\n",
    "\n",
    "#         if accuracy > best_accuracy:\n",
    "#             best_accuracy = accuracy\n",
    "#     #             pickle.dump([kmeans, svm], open(\"best.classifier.pckl\", \"wb\"))\n",
    "#             tries = 0\n",
    "#         else:\n",
    "#             tries += 1\n",
    "#             if tries >= patience:\n",
    "#     #                 [kmeans, svm] = pickle.load(open(\"best.classifier.pckl\", \"rb\"))\n",
    "#                 break      \n",
    "\n",
    "#         keep, split, deactivate = study_clusters(weights, uncertainty, cluster2words, cluster_weights_p=cluster_weights_p/10)\n",
    "#     #     keep, split, deactivate = study_clusters(svm.coef_[0], uncertainty, cluster2words)\n",
    "\n",
    "#         keep_count = sum(keep)\n",
    "#         split_count = sum(split)\n",
    "#         deactivate_count = sum(deactivate)\n",
    "\n",
    "\n",
    "#         before = len(words_list)\n",
    "#         assert before == len(words_count)\n",
    "#         assert before == words_embedding.shape[0]\n",
    "\n",
    "#         words_list, words_count, words_embedding = deactivate_clusters(word2idx, cluster2words, words_list, words_count, words_embedding, deactivate, split, keep)\n",
    "\n",
    "#         word2idx = {word: idx for idx, word in enumerate(words_list)}\n",
    "#         idx2word = dict(enumerate(words_list))\n",
    "\n",
    "#         word2cluster, next_n_clusters = split_clusters(words_embedding, word2idx, word2cluster, cluster2words, keep, split)    \n",
    "\n",
    "#         assert next_n_clusters <= n_clusters - deactivate_count + split_count    \n",
    "\n",
    "#         cluster2words = [[] for i in range(next_n_clusters)]\n",
    "#         for word in words_list:\n",
    "#             cluster2words[word2cluster[word]].append(word)\n",
    "\n",
    "#         after = len(words_list)\n",
    "#         assert after == len(words_count)\n",
    "#         assert after == words_embedding.shape[0]\n",
    "\n",
    "#         print_clusters(cluster2words, words_count, word2idx, print_cluster_words=False)\n",
    "\n",
    "#         history.append([\n",
    "#             i, \n",
    "#             n_clusters, \n",
    "#             keep_count, \n",
    "#             split_count,\n",
    "#             deactivate_count,\n",
    "#             before,\n",
    "#             after,\n",
    "#             accuracy,\n",
    "#             kappa\n",
    "#         ])\n",
    "\n",
    "#         n_clusters = next_n_clusters\n",
    "\n",
    "#     columns = [\n",
    "#     'Epoch',\n",
    "#     'Clusters', \n",
    "#     'Keep', \n",
    "#     'Split',\n",
    "#     'Deactivate',\n",
    "#     'Words before',\n",
    "#     'Words after',\n",
    "#     'Accuracy',\n",
    "#     'Kappa'\n",
    "#     ]\n",
    "\n",
    "#     df = DataFrame(history, columns=columns)\n",
    "#     display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cluster_weights_p in range(11):\n",
    "#     for low_uncertainty_p in [4, 5, 6, 7]:\n",
    "#         for high_uncertainty_p in [6, 7, 8]:\n",
    "            \n",
    "#             if high_uncertainty_p < low_uncertainty_p:\n",
    "#                 continue\n",
    "                \n",
    "#             epochs = 200\n",
    "\n",
    "#             patience = 5\n",
    "#             tries = 0\n",
    "#             best_accuracy = 0\n",
    "\n",
    "#             words_list = deepcopy(original_words_list)\n",
    "#             words_count = deepcopy(original_words_count)\n",
    "#             words_embedding = deepcopy(original_words_embedding)\n",
    "#             word2cluster = deepcopy(original_word2cluster)\n",
    "#             cluster2words = deepcopy(original_cluster2words)\n",
    "#             word2idx = deepcopy(original_word2idx)\n",
    "#             idx2word = deepcopy(original_idx2word)\n",
    "#             n_clusters = 50\n",
    "\n",
    "#             history = []\n",
    "\n",
    "#             print_clusters(cluster2words, words_count, word2idx, print_cluster_words=False)\n",
    "\n",
    "#             print('Using cluster_weights_p: %f' % (cluster_weights_p / 10))\n",
    "#             print('Using low_uncertainty_p: %f' % (low_uncertainty_p / 10))\n",
    "#             print('Using high_uncertainty_p: %f' % (high_uncertainty_p / 10))\n",
    "\n",
    "#             for i in range(epochs):\n",
    "#                 print('Epoch %d of %d' % (i + 1, epochs))\n",
    "#                 print('Number of clusters: %d' % (n_clusters))\n",
    "#                 bows_train = clusters_bow(x_train, n_clusters, word2cluster, useFrequency=False)\n",
    "#                 svm = fit_svm(bows_train, y_train)\n",
    "\n",
    "#                 weights, _ = chi2(bows_train, y_train)\n",
    "#                 uncertainty = clusters_uncertainty(bows_train, y_train)\n",
    "\n",
    "#             #     print(uncertainty)\n",
    "\n",
    "#                 bows_val = clusters_bow(x_val, n_clusters, word2cluster, useFrequency=False)    \n",
    "#                 accuracy, kappa = validate(svm, bows_val, y_val)\n",
    "\n",
    "#                 print('Accuracy: %.3f' % (accuracy))\n",
    "\n",
    "#                 if accuracy > best_accuracy:\n",
    "#                     best_accuracy = accuracy\n",
    "#             #             pickle.dump([kmeans, svm], open(\"best.classifier.pckl\", \"wb\"))\n",
    "#                     tries = 0\n",
    "#                 else:\n",
    "#                     tries += 1\n",
    "#                     if tries >= patience:\n",
    "#             #                 [kmeans, svm] = pickle.load(open(\"best.classifier.pckl\", \"rb\"))\n",
    "#                         break      \n",
    "\n",
    "#                 keep, split, deactivate = study_clusters(weights, \n",
    "#                                                          uncertainty, \n",
    "#                                                          cluster2words, \n",
    "#                                                          low_uncertainty_p=low_uncertainty_p/10, \n",
    "#                                                          high_uncertainty_p=high_uncertainty_p/10,\n",
    "#                                                          cluster_weights_p=cluster_weights_p/10)\n",
    "#             #     keep, split, deactivate = study_clusters(svm.coef_[0], uncertainty, cluster2words)\n",
    "\n",
    "#                 keep_count = sum(keep)\n",
    "#                 split_count = sum(split)\n",
    "#                 deactivate_count = sum(deactivate)\n",
    "\n",
    "\n",
    "#                 before = len(words_list)\n",
    "#                 assert before == len(words_count)\n",
    "#                 assert before == words_embedding.shape[0]\n",
    "\n",
    "#                 words_list, words_count, words_embedding = deactivate_clusters(word2idx, \n",
    "#                                                                                cluster2words, \n",
    "#                                                                                words_list, \n",
    "#                                                                                words_count, \n",
    "#                                                                                words_embedding, \n",
    "#                                                                                deactivate, \n",
    "#                                                                                split, \n",
    "#                                                                                keep)\n",
    "\n",
    "#                 word2idx = {word: idx for idx, word in enumerate(words_list)}\n",
    "#                 idx2word = dict(enumerate(words_list))\n",
    "\n",
    "#                 word2cluster, next_n_clusters = split_clusters(words_embedding, \n",
    "#                                                                word2idx, \n",
    "#                                                                word2cluster, \n",
    "#                                                                cluster2words, \n",
    "#                                                                keep, \n",
    "#                                                                split)    \n",
    "\n",
    "#                 assert next_n_clusters <= n_clusters - deactivate_count + split_count    \n",
    "\n",
    "#                 cluster2words = [[] for i in range(next_n_clusters)]\n",
    "#                 for word in words_list:\n",
    "#                     cluster2words[word2cluster[word]].append(word)\n",
    "\n",
    "#                 after = len(words_list)\n",
    "#                 assert after == len(words_count)\n",
    "#                 assert after == words_embedding.shape[0]\n",
    "\n",
    "#                 print_clusters(cluster2words, words_count, word2idx, print_cluster_words=False)\n",
    "\n",
    "#                 history.append([\n",
    "#                     i, \n",
    "#                     n_clusters, \n",
    "#                     keep_count, \n",
    "#                     split_count,\n",
    "#                     deactivate_count,\n",
    "#                     before,\n",
    "#                     after,\n",
    "#                     accuracy,\n",
    "#                     kappa\n",
    "#                 ])\n",
    "\n",
    "#                 n_clusters = next_n_clusters\n",
    "\n",
    "#             columns = [\n",
    "#             'Epoch',\n",
    "#             'Clusters', \n",
    "#             'Keep', \n",
    "#             'Split',\n",
    "#             'Deactivate',\n",
    "#             'Words before',\n",
    "#             'Words after',\n",
    "#             'Accuracy',\n",
    "#             'Kappa'\n",
    "#             ]\n",
    "\n",
    "#             df = DataFrame(history, columns=columns)\n",
    "#             display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cluster_weights_p: 0.800000\n",
      "Using low_uncertainty_p: 0.400000\n",
      "Using high_uncertainty_p: 0.600000\n",
      "Epoch 1 of 200\n",
      "Number of clusters: 150\n",
      "Accuracy: 0.887\n",
      "Epoch 2 of 200\n",
      "Number of clusters: 149\n",
      "Accuracy: 0.883\n",
      "Epoch 3 of 200\n",
      "Number of clusters: 147\n",
      "Accuracy: 0.894\n",
      "Epoch 4 of 200\n",
      "Number of clusters: 148\n",
      "Accuracy: 0.894\n",
      "Epoch 5 of 200\n",
      "Number of clusters: 158\n",
      "Accuracy: 0.889\n",
      "Epoch 6 of 200\n",
      "Number of clusters: 173\n",
      "Accuracy: 0.901\n",
      "Epoch 7 of 200\n",
      "Number of clusters: 193\n",
      "Accuracy: 0.902\n",
      "Epoch 8 of 200\n",
      "Number of clusters: 208\n",
      "Accuracy: 0.903\n",
      "Epoch 9 of 200\n",
      "Number of clusters: 220\n",
      "Accuracy: 0.903\n",
      "Epoch 10 of 200\n",
      "Number of clusters: 239\n",
      "Accuracy: 0.904\n",
      "Epoch 11 of 200\n",
      "Number of clusters: 251\n",
      "Accuracy: 0.903\n",
      "Epoch 12 of 200\n",
      "Number of clusters: 269\n",
      "Accuracy: 0.904\n",
      "Epoch 13 of 200\n",
      "Number of clusters: 286\n",
      "Accuracy: 0.905\n",
      "Epoch 14 of 200\n",
      "Number of clusters: 296\n",
      "Accuracy: 0.903\n",
      "Epoch 15 of 200\n",
      "Number of clusters: 304\n",
      "Accuracy: 0.907\n",
      "Epoch 16 of 200\n",
      "Number of clusters: 309\n",
      "Accuracy: 0.908\n",
      "Epoch 17 of 200\n",
      "Number of clusters: 313\n",
      "Accuracy: 0.908\n",
      "Epoch 18 of 200\n",
      "Number of clusters: 313\n",
      "Accuracy: 0.908\n",
      "Epoch 19 of 200\n",
      "Number of clusters: 306\n",
      "Accuracy: 0.903\n",
      "Epoch 20 of 200\n",
      "Number of clusters: 301\n",
      "Accuracy: 0.906\n",
      "Epoch 21 of 200\n",
      "Number of clusters: 298\n",
      "Accuracy: 0.905\n",
      "Epoch 22 of 200\n",
      "Number of clusters: 288\n",
      "Accuracy: 0.906\n",
      "Epoch 23 of 200\n",
      "Number of clusters: 277\n",
      "Accuracy: 0.907\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clusters</th>\n",
       "      <th>Min Length</th>\n",
       "      <th>P25 Length</th>\n",
       "      <th>Median Length</th>\n",
       "      <th>P75 Length</th>\n",
       "      <th>Max Length</th>\n",
       "      <th>Keep</th>\n",
       "      <th>Split</th>\n",
       "      <th>Deactivate</th>\n",
       "      <th>Words before</th>\n",
       "      <th>Words after</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Kappa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>26.75</td>\n",
       "      <td>21701.0</td>\n",
       "      <td>57</td>\n",
       "      <td>47</td>\n",
       "      <td>46</td>\n",
       "      <td>61651</td>\n",
       "      <td>61180</td>\n",
       "      <td>0.88695</td>\n",
       "      <td>0.773961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>17490.0</td>\n",
       "      <td>54</td>\n",
       "      <td>51</td>\n",
       "      <td>44</td>\n",
       "      <td>61180</td>\n",
       "      <td>60578</td>\n",
       "      <td>0.88340</td>\n",
       "      <td>0.766854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>9.0</td>\n",
       "      <td>44.00</td>\n",
       "      <td>15157.0</td>\n",
       "      <td>52</td>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>60578</td>\n",
       "      <td>59764</td>\n",
       "      <td>0.89425</td>\n",
       "      <td>0.788543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>175.25</td>\n",
       "      <td>11638.0</td>\n",
       "      <td>49</td>\n",
       "      <td>58</td>\n",
       "      <td>41</td>\n",
       "      <td>59764</td>\n",
       "      <td>59302</td>\n",
       "      <td>0.89390</td>\n",
       "      <td>0.787831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>158</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>21.0</td>\n",
       "      <td>241.00</td>\n",
       "      <td>6102.0</td>\n",
       "      <td>54</td>\n",
       "      <td>64</td>\n",
       "      <td>40</td>\n",
       "      <td>59302</td>\n",
       "      <td>57804</td>\n",
       "      <td>0.88925</td>\n",
       "      <td>0.778540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>173</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>25.0</td>\n",
       "      <td>230.00</td>\n",
       "      <td>4841.0</td>\n",
       "      <td>61</td>\n",
       "      <td>70</td>\n",
       "      <td>42</td>\n",
       "      <td>57804</td>\n",
       "      <td>57071</td>\n",
       "      <td>0.90065</td>\n",
       "      <td>0.801343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>53.0</td>\n",
       "      <td>219.00</td>\n",
       "      <td>3773.0</td>\n",
       "      <td>70</td>\n",
       "      <td>76</td>\n",
       "      <td>47</td>\n",
       "      <td>57071</td>\n",
       "      <td>53799</td>\n",
       "      <td>0.90160</td>\n",
       "      <td>0.803212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>208</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>41.0</td>\n",
       "      <td>180.00</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>76</td>\n",
       "      <td>81</td>\n",
       "      <td>51</td>\n",
       "      <td>53799</td>\n",
       "      <td>52205</td>\n",
       "      <td>0.90285</td>\n",
       "      <td>0.805717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>220</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>36.0</td>\n",
       "      <td>167.75</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>79</td>\n",
       "      <td>88</td>\n",
       "      <td>53</td>\n",
       "      <td>52205</td>\n",
       "      <td>50494</td>\n",
       "      <td>0.90320</td>\n",
       "      <td>0.806419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>239</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>27.0</td>\n",
       "      <td>143.00</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>86</td>\n",
       "      <td>93</td>\n",
       "      <td>60</td>\n",
       "      <td>50494</td>\n",
       "      <td>49178</td>\n",
       "      <td>0.90380</td>\n",
       "      <td>0.807615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>127.50</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>90</td>\n",
       "      <td>101</td>\n",
       "      <td>60</td>\n",
       "      <td>49178</td>\n",
       "      <td>47587</td>\n",
       "      <td>0.90265</td>\n",
       "      <td>0.805311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>269</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>109.00</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>99</td>\n",
       "      <td>106</td>\n",
       "      <td>64</td>\n",
       "      <td>47587</td>\n",
       "      <td>44872</td>\n",
       "      <td>0.90360</td>\n",
       "      <td>0.807226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>71.00</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>104</td>\n",
       "      <td>113</td>\n",
       "      <td>69</td>\n",
       "      <td>44872</td>\n",
       "      <td>43954</td>\n",
       "      <td>0.90525</td>\n",
       "      <td>0.810515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>14.5</td>\n",
       "      <td>62.50</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>103</td>\n",
       "      <td>116</td>\n",
       "      <td>77</td>\n",
       "      <td>43954</td>\n",
       "      <td>41563</td>\n",
       "      <td>0.90300</td>\n",
       "      <td>0.806014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>304</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>53.25</td>\n",
       "      <td>2806.0</td>\n",
       "      <td>110</td>\n",
       "      <td>117</td>\n",
       "      <td>77</td>\n",
       "      <td>41563</td>\n",
       "      <td>39607</td>\n",
       "      <td>0.90730</td>\n",
       "      <td>0.814618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>309</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47.00</td>\n",
       "      <td>2806.0</td>\n",
       "      <td>115</td>\n",
       "      <td>119</td>\n",
       "      <td>75</td>\n",
       "      <td>39607</td>\n",
       "      <td>38328</td>\n",
       "      <td>0.90825</td>\n",
       "      <td>0.816517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>313</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>45.00</td>\n",
       "      <td>2806.0</td>\n",
       "      <td>115</td>\n",
       "      <td>121</td>\n",
       "      <td>77</td>\n",
       "      <td>38328</td>\n",
       "      <td>37674</td>\n",
       "      <td>0.90800</td>\n",
       "      <td>0.816023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>313</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>42.00</td>\n",
       "      <td>2806.0</td>\n",
       "      <td>113</td>\n",
       "      <td>119</td>\n",
       "      <td>81</td>\n",
       "      <td>37674</td>\n",
       "      <td>36915</td>\n",
       "      <td>0.90830</td>\n",
       "      <td>0.816622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>306</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.5</td>\n",
       "      <td>40.50</td>\n",
       "      <td>2806.0</td>\n",
       "      <td>111</td>\n",
       "      <td>117</td>\n",
       "      <td>78</td>\n",
       "      <td>36915</td>\n",
       "      <td>34964</td>\n",
       "      <td>0.90295</td>\n",
       "      <td>0.805927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>37.00</td>\n",
       "      <td>2806.0</td>\n",
       "      <td>111</td>\n",
       "      <td>117</td>\n",
       "      <td>73</td>\n",
       "      <td>34964</td>\n",
       "      <td>33844</td>\n",
       "      <td>0.90610</td>\n",
       "      <td>0.812228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>37.00</td>\n",
       "      <td>2663.0</td>\n",
       "      <td>107</td>\n",
       "      <td>120</td>\n",
       "      <td>71</td>\n",
       "      <td>33844</td>\n",
       "      <td>33051</td>\n",
       "      <td>0.90540</td>\n",
       "      <td>0.810828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>35.25</td>\n",
       "      <td>2663.0</td>\n",
       "      <td>100</td>\n",
       "      <td>111</td>\n",
       "      <td>77</td>\n",
       "      <td>33051</td>\n",
       "      <td>31536</td>\n",
       "      <td>0.90575</td>\n",
       "      <td>0.811523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clusters  Min Length  P25 Length  Median Length  P75 Length  \\\n",
       "Epoch                                                                \n",
       "0           150         1.0        4.00            9.0       26.75   \n",
       "1           149         1.0        2.00            7.0       28.00   \n",
       "2           147         1.0        1.50            9.0       44.00   \n",
       "3           148         1.0        2.00           13.0      175.25   \n",
       "4           158         1.0        3.00           21.0      241.00   \n",
       "5           173         1.0        4.00           25.0      230.00   \n",
       "6           193         1.0        5.00           53.0      219.00   \n",
       "7           208         1.0        4.00           41.0      180.00   \n",
       "8           220         1.0        3.75           36.0      167.75   \n",
       "9           239         1.0        3.00           27.0      143.00   \n",
       "10          251         1.0        5.00           28.0      127.50   \n",
       "11          269         1.0        4.00           24.0      109.00   \n",
       "12          286         1.0        2.00           15.0       71.00   \n",
       "13          296         1.0        2.00           14.5       62.50   \n",
       "14          304         1.0        2.00           12.0       53.25   \n",
       "15          309         1.0        1.00           10.0       47.00   \n",
       "16          313         1.0        1.00            8.0       45.00   \n",
       "17          313         1.0        1.00            6.0       42.00   \n",
       "18          306         1.0        1.00            5.5       40.50   \n",
       "19          301         1.0        1.00            5.0       37.00   \n",
       "20          298         1.0        1.00            5.0       37.00   \n",
       "21          288         1.0        1.00            4.0       35.25   \n",
       "\n",
       "       Max Length  Keep  Split  Deactivate  Words before  Words after  \\\n",
       "Epoch                                                                   \n",
       "0         21701.0    57     47          46         61651        61180   \n",
       "1         17490.0    54     51          44         61180        60578   \n",
       "2         15157.0    52     55          40         60578        59764   \n",
       "3         11638.0    49     58          41         59764        59302   \n",
       "4          6102.0    54     64          40         59302        57804   \n",
       "5          4841.0    61     70          42         57804        57071   \n",
       "6          3773.0    70     76          47         57071        53799   \n",
       "7          3180.0    76     81          51         53799        52205   \n",
       "8          3180.0    79     88          53         52205        50494   \n",
       "9          3180.0    86     93          60         50494        49178   \n",
       "10         3180.0    90    101          60         49178        47587   \n",
       "11         3180.0    99    106          64         47587        44872   \n",
       "12         3180.0   104    113          69         44872        43954   \n",
       "13         3180.0   103    116          77         43954        41563   \n",
       "14         2806.0   110    117          77         41563        39607   \n",
       "15         2806.0   115    119          75         39607        38328   \n",
       "16         2806.0   115    121          77         38328        37674   \n",
       "17         2806.0   113    119          81         37674        36915   \n",
       "18         2806.0   111    117          78         36915        34964   \n",
       "19         2806.0   111    117          73         34964        33844   \n",
       "20         2663.0   107    120          71         33844        33051   \n",
       "21         2663.0   100    111          77         33051        31536   \n",
       "\n",
       "       Accuracy     Kappa  \n",
       "Epoch                      \n",
       "0       0.88695  0.773961  \n",
       "1       0.88340  0.766854  \n",
       "2       0.89425  0.788543  \n",
       "3       0.89390  0.787831  \n",
       "4       0.88925  0.778540  \n",
       "5       0.90065  0.801343  \n",
       "6       0.90160  0.803212  \n",
       "7       0.90285  0.805717  \n",
       "8       0.90320  0.806419  \n",
       "9       0.90380  0.807615  \n",
       "10      0.90265  0.805311  \n",
       "11      0.90360  0.807226  \n",
       "12      0.90525  0.810515  \n",
       "13      0.90300  0.806014  \n",
       "14      0.90730  0.814618  \n",
       "15      0.90825  0.816517  \n",
       "16      0.90800  0.816023  \n",
       "17      0.90830  0.816622  \n",
       "18      0.90295  0.805927  \n",
       "19      0.90610  0.812228  \n",
       "20      0.90540  0.810828  \n",
       "21      0.90575  0.811523  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster_weights_p = 8\n",
    "low_uncertainty_p = 4\n",
    "high_uncertainty_p = 6\n",
    "            \n",
    "\n",
    "epochs = 200\n",
    "\n",
    "patience = 5\n",
    "tries = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "words_list = deepcopy(original_words_list)\n",
    "words_count = deepcopy(original_words_count)\n",
    "words_embedding = deepcopy(original_words_embedding)\n",
    "word2cluster = deepcopy(original_word2cluster)\n",
    "cluster2words = deepcopy(original_cluster2words)\n",
    "word2idx = deepcopy(original_word2idx)\n",
    "idx2word = deepcopy(original_idx2word)\n",
    "n_clusters = 150\n",
    "\n",
    "history = []\n",
    "\n",
    "# print_clusters(cluster2words, words_count, word2idx, print_cluster_words=False)\n",
    "\n",
    "print('Using cluster_weights_p: %f' % (cluster_weights_p / 10))\n",
    "print('Using low_uncertainty_p: %f' % (low_uncertainty_p / 10))\n",
    "print('Using high_uncertainty_p: %f' % (high_uncertainty_p / 10))\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('Epoch %d of %d' % (i + 1, epochs))\n",
    "    print('Number of clusters: %d' % (n_clusters))\n",
    "    \n",
    "    # print_clusters(cluster2words, words_count, word2idx, print_cluster_words=False)\n",
    "    clusters_len = [len(cluster2words[i]) for i in range(len(cluster2words))]\n",
    "    [pmin, p25, p50, p75, pmax] = np.percentile(clusters_len, [0, 25, 50, 75, 100])\n",
    "    \n",
    "    bows_train = clusters_bow(x_train, n_clusters, word2cluster, useFrequency=False)\n",
    "    svm = fit_svm(bows_train, y_train)\n",
    "\n",
    "    weights, _ = chi2(bows_train, y_train)\n",
    "    uncertainty = clusters_uncertainty(bows_train, y_train)\n",
    "\n",
    "#     print(uncertainty)\n",
    "\n",
    "    bows_val = clusters_bow(x_val, n_clusters, word2cluster, useFrequency=False)    \n",
    "    accuracy, kappa = validate(svm, bows_val, y_val)\n",
    "\n",
    "    print('Accuracy: %.3f' % (accuracy))\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "#             pickle.dump([kmeans, svm], open(\"best.classifier.pckl\", \"wb\"))\n",
    "        tries = 0\n",
    "    else:\n",
    "        tries += 1\n",
    "        if tries >= patience:\n",
    "#                 [kmeans, svm] = pickle.load(open(\"best.classifier.pckl\", \"rb\"))\n",
    "            break      \n",
    "\n",
    "    keep, split, deactivate = study_clusters(weights, \n",
    "                                             uncertainty, \n",
    "                                             cluster2words, \n",
    "                                             low_uncertainty_p=low_uncertainty_p/10, \n",
    "                                             high_uncertainty_p=high_uncertainty_p/10,\n",
    "                                             cluster_weights_p=cluster_weights_p/10)\n",
    "#     keep, split, deactivate = study_clusters(svm.coef_[0], uncertainty, cluster2words)\n",
    "\n",
    "    keep_count = sum(keep)\n",
    "    split_count = sum(split)\n",
    "    deactivate_count = sum(deactivate)\n",
    "\n",
    "\n",
    "    before = len(words_list)\n",
    "    assert before == len(words_count)\n",
    "    assert before == words_embedding.shape[0]\n",
    "\n",
    "    words_list, words_count, words_embedding = deactivate_clusters(word2idx, \n",
    "                                                                   cluster2words, \n",
    "                                                                   words_list, \n",
    "                                                                   words_count, \n",
    "                                                                   words_embedding, \n",
    "                                                                   deactivate, \n",
    "                                                                   split, \n",
    "                                                                   keep)\n",
    "\n",
    "    word2idx = {word: idx for idx, word in enumerate(words_list)}\n",
    "    idx2word = dict(enumerate(words_list))\n",
    "\n",
    "    word2cluster, next_n_clusters = split_clusters(words_embedding, \n",
    "                                                   word2idx, \n",
    "                                                   word2cluster, \n",
    "                                                   cluster2words, \n",
    "                                                   keep, \n",
    "                                                   split)    \n",
    "\n",
    "    assert next_n_clusters <= n_clusters - deactivate_count + split_count    \n",
    "\n",
    "    cluster2words = [[] for i in range(next_n_clusters)]\n",
    "    for word in words_list:\n",
    "        cluster2words[word2cluster[word]].append(word)\n",
    "\n",
    "    after = len(words_list)\n",
    "    assert after == len(words_count)\n",
    "    assert after == words_embedding.shape[0]\n",
    "\n",
    "    history.append([\n",
    "        i, \n",
    "        n_clusters,\n",
    "        pmin,\n",
    "        p25,\n",
    "        p50, \n",
    "        p75,\n",
    "        pmax,\n",
    "        keep_count, \n",
    "        split_count,\n",
    "        deactivate_count,\n",
    "        before,\n",
    "        after,\n",
    "        accuracy,\n",
    "        kappa\n",
    "    ])\n",
    "\n",
    "    n_clusters = next_n_clusters\n",
    "\n",
    "columns = [\n",
    "'Epoch',\n",
    "'Clusters', \n",
    "'Min Length',\n",
    "'P25 Length',\n",
    "'Median Length',\n",
    "'P75 Length',\n",
    "'Max Length',\n",
    "'Keep', \n",
    "'Split',\n",
    "'Deactivate',\n",
    "'Words before',\n",
    "'Words after',\n",
    "'Accuracy',\n",
    "'Kappa'\n",
    "]\n",
    "\n",
    "df = DataFrame(history, columns=columns).set_index('Epoch')\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
