{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleccionando palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import reuters_reader\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import Counter\n",
    "from sklearn.externals import joblib\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from scipy.sparse import csr_matrix\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from pandas import DataFrame\n",
    "from IPython.display import display\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCV1 Dataset\n",
    "\n",
    "Use the function ```reuters_reader.reader(path)``` to retrieve the available documents from the rcv1 dataset stored in `path`. This function returns a generator (```reader```) which yields a single document each time we call ```next(reader)```. Each document is a dictionary with the followitn useful keys:\n",
    " - \"title\" is the title of the document\n",
    " - \"text\" is the body of the document\n",
    " - \"bip:topics:1.0\" is the list of topics\n",
    " \n",
    "There are a total of 804420 available documents, although some may have no topic.\n",
    "\n",
    "#### Building the dataset\n",
    "We build a balanced dataset that contains ```n_docs```. To get a balanced dataset we iterate through the documents generator until we have ```n_docs / 2``` documents with the desired topic and the same amount without it. \n",
    "\n",
    "#### Get the labels\n",
    "Select a topic we want to classify using the variable topic (there is a list of the topics https://gist.github.com/gavinmh/6253739). Then build the list of labels using a 1 for those documents with that topic and 0 otherwise\n",
    "\n",
    "#### Training and validation set\n",
    "Finally, we split the dataset using the ```train_split``` value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19960917 / 59589newsML.xml failed to parse XML.\n",
      "28464 docs with topic GCAT (from 100000)\n",
      "Training with 80000 docs\n",
      "Validating with 20000 docs\n"
     ]
    }
   ],
   "source": [
    "path = 'rcv1'\n",
    "n_docs = 100000\n",
    "train_split = 0.8\n",
    "topic = 'GCAT'\n",
    "\n",
    "docs = []\n",
    "reader = reuters_reader.reader(path)\n",
    "\n",
    "topic_true = 0\n",
    "topic_false = 0\n",
    "\n",
    "while len(docs) < n_docs:\n",
    "    doc = next(reader)\n",
    "    docs.append(doc)\n",
    "     \n",
    "random.shuffle(docs)\n",
    "\n",
    "labels = np.zeros((n_docs), dtype=np.int16)\n",
    "labels = [1 if topic in doc['bip:topics:1.0'] else 0 for doc in docs]\n",
    "\n",
    "print('{} docs with topic {} (from {})'.format(np.sum(labels), topic, n_docs))\n",
    "\n",
    "split_point = int(n_docs * train_split)\n",
    "x_train, y_train = docs[:split_point], labels[:split_point]\n",
    "x_val, y_val = docs[split_point:], labels[split_point:]\n",
    "\n",
    "print('Training with {} docs'.format(len(x_train)))\n",
    "print('Validating with {} docs'.format(len(x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec model\n",
    "\n",
    "We are loading the well known word2vec model from __[Google](https://code.google.com/archive/p/word2vec/)__ which is stored in the binary file `GoogleNews-vectors-negative300.bin`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_name = 'GoogleNews-vectors-negative300.bin'\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_name, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our vocabulary\n",
    "\n",
    "Get all the the vectors from the word2vec for our vocabulary. Our vocabulary can include all the words used in the word2vec model or be limited to the words in our dataset.\n",
    "\n",
    "We can change this behaviour with the flag ```dataset_vocabulary```. ```False``` will use all the words from the word2vec model and ```True``` will limit them to just the words that are in our dataset and in the model at the same time.\n",
    "\n",
    "There is a ```count_threshold``` to remove those words appearing very few times because they are probably errors.\n",
    "\n",
    "As we have to split each documents in individual words, we already save this inside each document with the key \"counter\".\n",
    "\n",
    "After this cell, ```X``` is a matrix including all the vectors we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 56631\n"
     ]
    }
   ],
   "source": [
    "words_list = []\n",
    "words_embedding = []\n",
    "words_count = []\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "word2cluster = {}\n",
    "cluster2words = {}\n",
    "\n",
    "count_threshold = 5\n",
    "\n",
    "for doc in docs:\n",
    "    doc[\"counter\"] = Counter()\n",
    "    doc[\"word_count\"] = 0\n",
    "    words = doc[\"text\"].split()\n",
    "    words = [word.strip(string.punctuation) for word in words]\n",
    "    for word in words:\n",
    "        if word in w2v:\n",
    "            doc[\"counter\"][word] += 1\n",
    "            doc[\"word_count\"] += 1\n",
    "    for word, count in doc[\"counter\"].items():\n",
    "        try:\n",
    "            words_count[word2idx[word]] += count\n",
    "        except:\n",
    "            words_list.append(word)\n",
    "            words_count.append(count)\n",
    "            word2idx[word] = len(words_list) - 1\n",
    "                \n",
    "keep_it = [count > count_threshold for count in words_count] \n",
    "    \n",
    "words_list = [word for idx, word in enumerate(words_list) if keep_it[idx]]\n",
    "words_count = [count for idx, count in enumerate(words_count) if keep_it[idx]]\n",
    "word2idx = {word: idx for idx, word in enumerate(words_list)}\n",
    "    \n",
    "words_embedding = np.zeros((len(words_list), w2v.vector_size), dtype=np.float32)\n",
    "for idx, word in enumerate(words_list):\n",
    "    words_embedding[idx, :] += w2v[word]\n",
    "    \n",
    "idx2word = dict(enumerate(words_list))\n",
    "\n",
    "print(\"Vocabulary length: {}\".format(len(words_list)))\n",
    "\n",
    "for _ in range(100):\n",
    "    idx = random.randint(0, len(words_list) - 1)\n",
    "    word = words_list[idx]\n",
    "    assert(word2idx[word] == idx)\n",
    "    assert((w2v[word] == words_embedding[idx]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indptr = [0]\n",
    "indices = []\n",
    "data = []\n",
    "\n",
    "for doc in docs:\n",
    "    for word in doc[\"counter\"]:\n",
    "        if word in word2idx:\n",
    "            indices.append(word2idx[word])\n",
    "            data.append(1)\n",
    "    indptr.append(len(indices))\n",
    "    \n",
    "matrix = csr_matrix((data, indices, indptr), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svm(X_train, y_train, grid_search=False):\n",
    "    if grid_search:\n",
    "        c_parameters = [2 ** i for i in range(-15, 15)]\n",
    "#     print(c_parameters)\n",
    "        tuned_parameters = [{'C': c_parameters}]\n",
    "        clf = GridSearchCV(LinearSVC(dual=False), tuned_parameters, cv=10,\n",
    "                           scoring='accuracy', return_train_score=True)\n",
    "        clf.fit(X_train, y_train)    \n",
    "    else:\n",
    "        features2c = {50: 8, 100: 8, 150: 8, 200: 8, 250: 1, 300: 0.5, 350: 0.25, 400: 0.25, \n",
    "                    500: 0.25, 750: 0.25, 1000: 0.03, 1500: 0.03, 2000: 0.015625, 5000: 0.0078125, 10000: 0.0078125}\n",
    "        n_features = X_train.shape[1]\n",
    "        \n",
    "        features_thresholds = sorted(list(features2c.keys()))\n",
    "        for i, val in enumerate(features_thresholds):\n",
    "            if n_features < val:\n",
    "                break\n",
    "\n",
    "        if i == 0:\n",
    "            selected_c = features2c[features_thresholds[0]]\n",
    "        elif i == len(features_thresholds) - 1:\n",
    "            selected_c = features2c[features_thresholds[len(features_thresholds) - 1]]\n",
    "        else:\n",
    "            left_threshold = features_thresholds[i - 1]\n",
    "            right_threshold = features_thresholds[i]\n",
    "            if n_features - left_threshold > right_threshold - n_features:\n",
    "                selected_c = features2c[features_thresholds[i]]\n",
    "            else:\n",
    "                selected_c = features2c[features_thresholds[i - 1]]\n",
    "        \n",
    "        clf = LinearSVC(dual=False, C=selected_c)\n",
    "        clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lasso(bows, labels, alpha=None, verbose=False):\n",
    "    if alpha:\n",
    "        clf = linear_model.Lasso(alpha=alpha, positive=True)\n",
    "    else:\n",
    "        clf = linear_model.LassoCV(positive=True)\n",
    "    clf.fit(bows, labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Lasso coefficients: %s' % (np.array2string(clf.coef_, suppress_small=True)))\n",
    "        \n",
    "    if alpha:\n",
    "        return clf, clf.alpha_\n",
    "    else:\n",
    "        return clf, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(classifier, bows, y_true, threshold=None):\n",
    "    y_predicted = classifier.predict(bows)\n",
    "    if threshold is None:\n",
    "        threshold = np.mean(y_predicted)\n",
    "    y_predicted = [1 if i > threshold else 0 for i in y_predicted]\n",
    "    accuracy = accuracy_score(y_val, y_predicted)\n",
    "    kappa = cohen_kappa_score(y_val, y_predicted)\n",
    "    \n",
    "    print('Accuracy: %.3f' % (accuracy))\n",
    "    print('Kappa: %.3f' % (kappa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 50 features\n",
      "Using lasso classifier:\n",
      "Accuracy: 0.847\n",
      "Kappa: 0.629\n",
      "Using SVM classifier:\n",
      "Accuracy: 0.871\n",
      "Kappa: 0.653\n",
      "\n",
      "Using 100 features\n",
      "Using lasso classifier:\n",
      "Accuracy: 0.870\n",
      "Kappa: 0.682\n",
      "Using SVM classifier:\n",
      "Accuracy: 0.894\n",
      "Kappa: 0.722\n",
      "\n",
      "Using 150 features\n",
      "Using lasso classifier:\n",
      "Accuracy: 0.882\n",
      "Kappa: 0.713\n",
      "Using SVM classifier:\n",
      "Accuracy: 0.907\n",
      "Kappa: 0.758\n",
      "\n",
      "Using 200 features\n",
      "Using lasso classifier:\n",
      "Accuracy: 0.886\n",
      "Kappa: 0.722\n",
      "Using SVM classifier:\n",
      "Accuracy: 0.915\n",
      "Kappa: 0.778\n",
      "\n",
      "Using 250 features\n",
      "Using lasso classifier:\n",
      "Accuracy: 0.890\n",
      "Kappa: 0.734\n",
      "Using SVM classifier:\n",
      "Accuracy: 0.919\n",
      "Kappa: 0.790\n",
      "\n",
      "Using 300 features\n",
      "Using lasso classifier:\n",
      "Accuracy: 0.892\n",
      "Kappa: 0.737\n",
      "Using SVM classifier:\n",
      "Accuracy: 0.923\n",
      "Kappa: 0.801\n",
      "\n",
      "Using 350 features\n",
      "Using lasso classifier:\n",
      "Accuracy: 0.897\n",
      "Kappa: 0.749\n",
      "Using SVM classifier:\n",
      "Accuracy: 0.926\n",
      "Kappa: 0.810\n",
      "\n",
      "Using 400 features\n",
      "Using lasso classifier:\n",
      "Accuracy: 0.897\n",
      "Kappa: 0.750\n",
      "Using SVM classifier:\n",
      "Accuracy: 0.929\n",
      "Kappa: 0.818\n",
      "\n",
      "Using 500 features\n",
      "Using lasso classifier:\n",
      "Accuracy: 0.899\n",
      "Kappa: 0.756\n",
      "Using SVM classifier:\n",
      "Accuracy: 0.932\n",
      "Kappa: 0.825\n",
      "\n",
      "Using 750 features\n",
      "Using lasso classifier:\n",
      "Accuracy: 0.903\n",
      "Kappa: 0.764\n",
      "Using SVM classifier:\n",
      "Accuracy: 0.935\n",
      "Kappa: 0.833\n",
      "\n",
      "Using 1000 features\n",
      "Using lasso classifier:\n",
      "Accuracy: 0.907\n",
      "Kappa: 0.774\n",
      "Using SVM classifier:\n",
      "Accuracy: 0.940\n",
      "Kappa: 0.848\n",
      "\n",
      "Using 1500 features\n",
      "Using lasso classifier:\n",
      "Accuracy: 0.910\n",
      "Kappa: 0.783\n",
      "Using SVM classifier:\n",
      "Accuracy: 0.943\n",
      "Kappa: 0.856\n",
      "\n",
      "Using 2000 features\n",
      "Using lasso classifier:\n",
      "Accuracy: 0.914\n",
      "Kappa: 0.794\n",
      "Using SVM classifier:\n",
      "Accuracy: 0.946\n",
      "Kappa: 0.863\n",
      "\n",
      "Using 5000 features\n",
      "Using lasso classifier:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0ae385b8bc75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using lasso classifier:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlasso\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_lasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlasso\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_x_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_y_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9f2c22a69f3e>\u001b[0m in \u001b[0;36mfit_lasso\u001b[0;34m(bows, labels, alpha, verbose)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLassoCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/words/lib/python3.5/site-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 for train, test in folds)\n\u001b[1;32m   1191\u001b[0m         mse_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0;32m-> 1192\u001b[0;31m                              backend=\"threading\")(jobs)\n\u001b[0m\u001b[1;32m   1193\u001b[0m         \u001b[0mmse_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_l1_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0mmean_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/words/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/words/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/words/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/words/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/words/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/words/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/words/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/words/lib/python3.5/site-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36m_path_residuals\u001b[0;34m(X, y, train, test, path, path_params, alphas, l1_ratio, X_order, dtype)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;31m# X is copied and a reference is kept here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m     \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpath_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/words/lib/python3.5/site-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36mlasso_path\u001b[0;34m(X, y, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, **params)\u001b[0m\n\u001b[1;32m    263\u001b[0m                      \u001b[0malphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                      \u001b[0mcopy_X\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                      positive=positive, return_n_iter=return_n_iter, **params)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/words/lib/python3.5/site-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36menet_path\u001b[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_sparse_scaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                 max_iter, tol, rng, random, positive)\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             model = cd_fast.enet_coordinate_descent_multi_task(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_search = False\n",
    "\n",
    "scores, _ = chi2(matrix, labels)\n",
    "sorted_idx = np.argsort(scores, kind=\"mergesort\")\n",
    "\n",
    "split_point = int(n_docs * train_split)\n",
    "\n",
    "features = [50, 100, 150, 200, 250, 300, 350, 400, 500, 750, 1000, 1500, 2000, 5000, 10000, 'all']\n",
    "\n",
    "for number_features in features:\n",
    "    print(\"Using %s features\" % (str(number_features)))\n",
    "    \n",
    "    if number_features != 'all':\n",
    "        idx = sorted_idx[-number_features:]\n",
    "    else:\n",
    "        idx = list(range(matrix.shape[1]))\n",
    "    \n",
    "    baseline_x_train, baseline_y_train = matrix[:split_point, idx], labels[:split_point]\n",
    "    baseline_x_val, baseline_y_val = matrix[split_point:, idx], labels[split_point:]\n",
    "\n",
    "    print(\"Using lasso classifier:\")\n",
    "    lasso, alpha = fit_lasso(baseline_x_train, baseline_y_train)\n",
    "    validate(lasso, baseline_x_val, baseline_y_val)\n",
    "    \n",
    "    print(\"Using SVM classifier:\")\n",
    "    svm_classifier = fit_svm(baseline_x_train, baseline_y_train, grid_search=grid_search)\n",
    "    if grid_search:\n",
    "        df = DataFrame(svm_classifier.cv_results_ )\n",
    "        display(df[['param_C', 'rank_test_score']])\n",
    "    validate(svm_classifier, baseline_x_val, baseline_y_val)\n",
    "    \n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
