{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleccionando palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import reuters_reader\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import Counter\n",
    "from sklearn.externals import joblib\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn import linear_model\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCV1 Dataset\n",
    "\n",
    "Use the function ```reuters_reader.reader(path)``` to retrieve the available documents from the rcv1 dataset stored in `path`. This function generates a document which is a dictionary with the data from an specific rcv1 entry. There are a total of 804420 available documents, although some may have no topic.\n",
    "\n",
    "Some useful keys in this dicitionary are:\n",
    " - \"title\"\n",
    " - \"text\" which is the body of document\n",
    " - \"bip:topics:1.0\" is a list of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: 500000\n",
      "None\n",
      "['C12', 'CCAT', 'GCAT', 'GCRIM']\n",
      "South African Airways (SAA) SAA.CN has accused domestic rival Comair, whose new franchise agreement   had obtained much more detailed information on passengers.\n",
      "\n",
      "- Johannesburg newsroom +27 11 482 1003\n",
      "None\n",
      "['C11', 'CCAT']\n",
      "German retail group Karstadt AG plans to move to a holding structure as soon as it completes its int .2 million marks in 1995 from a depressed 41.9 million in 1994.\n",
      "\n",
      "--Frankfurt Newsroom, +49 69 756525\n",
      "None\n",
      "['E12', 'ECAT', 'GCAT', 'GPOL']\n",
      "An Argentine bishop on Saturday attacked government plans to reform the labour market and said what  piness and creativity, that our pensioners lead a languid existence, nearly like a prolonged agony.\"\n",
      "None\n",
      "['GCAT', 'GVIO']\n",
      "A Jewish settler fired shots at Arab houses in the West Bank town of Hebron on Sunday but caused no  \n",
      "\n",
      "Settlers have said a partial Israeli army pullout from the town would put their lives in jeopardy.\n",
      "None\n",
      "['E12', 'E21', 'E211', 'ECAT', 'GCAT', 'GPOL']\n",
      "Prime Minister Rafik al-Hariri said his new government would not increase personal and corporate tax \n",
      "\n",
      "Hariri made no mention of Value Added Tax.\n",
      "\n",
      "- Beirut editorial (961 1) 864148 602057 353078 645456\n",
      "None\n",
      "['GCAT', 'GVIO']\n",
      "Anti-Taleban forces bombed Kabul for the second night running on Sunday, sending another big barrage were no reports of casualties or serious damage and the Taleban said the bombers missed the airport.\n",
      "None\n",
      "['E21', 'E211', 'ECAT', 'GCAT', 'GPOL']\n",
      "German Finance Minister Theo Waigel said in an interview to be published on Monday that it had not b  agreement and that's the way it will be,\" he said.\n",
      "\n",
      "-- Kevin Liffey, Bonn newsroom +49-228-26097160\n",
      "None\n",
      "['C42', 'CCAT', 'E41', 'E411', 'ECAT', 'GCAT', 'GJOB']\n",
      "Saudi Arabia's minister of labour told private companies to meet a November 11 deadline to increase   treasurer, auctioneer and customs clearance officer to secretaries, messengers and security guards.\n",
      "None\n",
      "['GCAT', 'GDIP']\n",
      "Eritrean Foreign Minister Petros Solomon arrived on Sunday on an official visit to Algeria to boost  s, accompanied by a large delegation, was greeted by his Algerian counterpart Ahmed Attaf, said APS.\n",
      "None\n",
      "['GCAT', 'GVIO']\n",
      "Forces belonging to the ousted Afghan government launched an attack on Sunday using rockets, heavy a le were lightly wounded,\" said a representative with the aid organisation, Medecins Sans Frontieres.\n",
      "19961028 / 146544newsML.xml failed to parse XML.\n",
      "19970601 / 629003newsML.xml failed to parse XML.\n",
      "19970725 / 756041newsML.xml failed to parse XML.\n",
      "19961218 / 265962newsML.xml failed to parse XML.\n",
      "19970203 / 350491newsML.xml failed to parse XML.\n",
      "19960917 / 59589newsML.xml failed to parse XML.\n"
     ]
    }
   ],
   "source": [
    "n_docs = 50000\n",
    "docs = []\n",
    "reader = reuters_reader.reader('rcv1')\n",
    "#total_documents = 0\n",
    "#for doc in reader:\n",
    "#    total_documents += 1\n",
    "#print('Total documents: {}'.format(total_documents))\n",
    "print('Using: {}'.format(n_docs))\n",
    "for i in range(n_docs):\n",
    "    doc = next(reader)\n",
    "    docs.append(doc)\n",
    "    if i < 10:\n",
    "        print(doc['title'])\n",
    "        print(doc['bip:topics:1.0'])\n",
    "        print(doc['text'][:100], doc['text'][-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec model\n",
    "\n",
    "We are going to load a well known word2vec model from __[Google](https://code.google.com/archive/p/word2vec/)__ which is stored in the binary file `GoogleNews-vectors-negative300.bin`.\n",
    "\n",
    "With the model we define a numpy matrix `X`, which contains as much vectors as words are in the word2vec vocabulary. We need this dataset to train the kmeans algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_name = 'GoogleNews-vectors-negative300.bin'\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_name, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our vocabulary\n",
    "\n",
    "Get all the the vectors from the word2vec for our vocabulary. Our vocabulary can include all the words used in the word2vec model or be limited to the words in our dataset.\n",
    "\n",
    "We can change the beahivour with the flag ```dataset_vocabulary```. ```False``` will use all the words from the word2vec model and ```True``` will limit them to just the words that are in our dataset and in the model at the same time.\n",
    "\n",
    "There is a ```count_threshold``` to remove those words appearing very few times because they are errors.\n",
    "\n",
    "After this cell, ```X``` is a matrix including all the vectors we are going to use. We also store the words for each document in ```doc[\"counter\"]```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vocabulary = True\n",
    "count_threshold = 5\n",
    "\n",
    "if dataset_vocabulary:\n",
    "    vocab = Counter()\n",
    "    for doc in docs:\n",
    "        doc[\"counter\"] = Counter()\n",
    "        words = doc[\"text\"].split()\n",
    "        words = [word.strip(string.punctuation) for word in words]\n",
    "        for word in words:\n",
    "            if word in w2v:\n",
    "                doc[\"counter\"][word] += 1\n",
    "                vocab[word] += 1\n",
    "    vocab = {word: count for word, count in vocab.items() if count > count_threshold}\n",
    "else:\n",
    "    vocab = w2v.index2word\n",
    "\n",
    "X = np.zeros((len(vocab), w2v.vector_size), dtype=np.float32)\n",
    "for index, word in enumerate(vocab):\n",
    "    X[index, :] += w2v[word]\n",
    "    \n",
    "print(\"Vocabulary length: {}\".format(X.shape[0]))\n",
    "print(\"Vector length: {}\".format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the most common and uncommon words in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Counter(vocab).most_common()[:10])\n",
    "print(Counter(vocab).most_common()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the labels ###\n",
    "Select a topic we want to classify using the variable ```topic``` (there is a list of the topics __[here](https://gist.github.com/gavinmh/6253739)__). Then build the list of labels using a ```1``` for those documents with that topic and 0 otherwise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'ECAT'\n",
    "\n",
    "labels = np.zeros((n_docs), dtype=np.int16)\n",
    "labels = [1 if topic in doc['bip:topics:1.0'] else 0 for doc in docs]\n",
    "\n",
    "print('{} docs with topic {} (from {})'.format(np.sum(labels), topic, n_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Cluster\n",
    "\n",
    "Train the first Kmeans cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 50\n",
    "kmeans_name = 'kmeans' + str(n_clusters) + '.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MiniBatchKMeans(n_clusters=n_clusters, random_state=0, compute_labels=True)\n",
    "classifier.fit(X)\n",
    "joblib.dump(classifier, kmeans_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_clusters(classifier, w2v, topn=3):\n",
    "# #     w2v.init_sims(replace=True)\n",
    "#     for (i, center) in enumerate(classifier.cluster_centers_):\n",
    "#         #print(center)\n",
    "#         similar = w2v.wv.most_similar_cosmul(positive=[center], topn=topn)\n",
    "#         similar = [t[0] for t in similar]\n",
    "#         print(\"Cluster %d: %s\" % (i + 1, ', '.join(similar)))\n",
    "        \n",
    "#print_clusters(classifier, w2v)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a bow model \n",
    "\n",
    "Using the clusters from the kmeans classifier build a bow for each document. This bag of words can be normalized usign the frequency of each word with ```useFrequency=True```. \n",
    "\n",
    "How? \n",
    "* For each document\n",
    "    * For each word\n",
    "        * Obtain the w2v vector for that word\n",
    "        * Obtain the cluster for that vector\n",
    "        * Add 1 to that cluster in the document bow\n",
    "\n",
    "To improve the performance the cluster for each word is saved in a dictionary. This way for each word we first check that dictionary instead of the model and then the classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_bow(docs, classifier, w2v, useFrequency=True, printing=False):\n",
    "    n_docs = len(docs)\n",
    "    n_clusters = classifier.n_clusters\n",
    "    bo_clusters = np.zeros((n_docs, n_clusters))\n",
    "\n",
    "    hashed_clusters = {}\n",
    "    for (i, doc) in enumerate(docs):\n",
    "        for word, count in doc[\"counter\"].items():\n",
    "            if word in hashed_clusters:\n",
    "                cluster = hashed_clusters[word]\n",
    "            else:\n",
    "                cluster = classifier.predict([w2v[word]])[0]\n",
    "                hashed_clusters[word] = cluster\n",
    "            bo_clusters[i][cluster] += count\n",
    "            \n",
    "    if useFrequency:\n",
    "        sums = np.sum(bo_clusters, axis=1)\n",
    "        boc = bo_clusters / sums.reshape((sums.shape[0], 1))\n",
    "    else:\n",
    "        idx = bo_clusters != 0\n",
    "        # Add a small amount to the denominator to avoid zero division\n",
    "        boc = np.round(bo_clusters / (bo_clusters + 0.001))\n",
    "        \n",
    "    if printing:\n",
    "        for i in range(10):\n",
    "            print('Document %d: %s' % (i + 1, np.array2string(boc[i, :])))\n",
    "        \n",
    "    return boc\n",
    "\n",
    "# bows = clusters_bow(docs, classifier, w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these bows to classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep, split or discard clusters\n",
    "\n",
    "Once we have our bags of words we can study how good are our clusters. To do so we use a Lasso function feeding it with the bows. \n",
    "\n",
    "From this function we obtain different coefficients that we will use to decide how good is a particular cluster. Only clusters with a coefficient higher than ```deactivate_value``` are kept, although between these some must be splitted. Splitted clusters are those with a coefficient lower than ```split_threshold``` times the maximum coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_clusters(bows, labels, deactivate_value=0.1, split_threshold=0.6, printing=False):  \n",
    "    clf = linear_model.LassoCV()\n",
    "    clf.fit(bows, labels)\n",
    "    \n",
    "    max_w = np.max(np.abs(clf.coef_))\n",
    "    split_value = deactivate_value * max_w\n",
    "\n",
    "    if printing:\n",
    "        print('Lasso coefficients: %s' % (np.array2string(clf.coef_, suppress_small=True)))\n",
    "        print('Split value (using threshold %.2f): %.2f' % (split_threshold, split_value))\n",
    "    \n",
    "    # deactivate = [x <= deactivate_value for x in np.abs(clf.coef_)]\n",
    "    split = [x > deactivate_value and x <= split_value for x in np.abs(clf.coef_)]\n",
    "    keep = [x > split_value for x in np.abs(clf.coef_)]\n",
    "    \n",
    "    return keep, split\n",
    "\n",
    "# keep, split = study_clusters(bows, labels)\n",
    "\n",
    "# print('Keeping %d clusters' % (len([x for x in keep if x == True])))\n",
    "# print('Spliting %d clusters' % (len([x for x in split if x == True])))\n",
    "\n",
    "# new_count = len([x for x in keep if x == True]) + 2 * len([x for x in split if x == True])\n",
    "# print('Using %d clusters in the next iteration' % (new_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know which clusters to keep and which to split, we can build the new ones. \n",
    "\n",
    "To split one cluster we need to select all the points belonging to that cluster and classify then using 2 neighbourds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centers(classifier, X, keep, split):\n",
    "    n_centers = len([x for x in keep if x == True]) + 2 * len([x for x in split if x == True])\n",
    "    updated_centers = np.empty((n_centers, classifier.cluster_centers_.shape[1]))\n",
    "    new_idx = 0\n",
    "    for i in range(len(keep)):\n",
    "        if keep[i]:\n",
    "            updated_centers[new_idx, :] = classifier.cluster_centers_[i, :]\n",
    "            new_idx += 1\n",
    "        if split[i]:\n",
    "            # create classifier with this data\n",
    "            newX = X[np.where(classifier.labels_ == i)[0], :]\n",
    "            small_class = MiniBatchKMeans(n_clusters=2, random_state=0, compute_labels=True)\n",
    "            small_class.fit(newX)\n",
    "            updated_centers[new_idx, :] = small_class.cluster_centers_[0, :]\n",
    "            updated_centers[new_idx + 1, :] = small_class.cluster_centers_[1, :]\n",
    "            new_idx += 2\n",
    "            \n",
    "    return updated_centers\n",
    "        \n",
    "# new_centers = update_centers(classifier, X, keep, split)\n",
    "    \n",
    "# print(classifier.cluster_centers_)\n",
    "# print(new_centers)\n",
    "# print(classifier.counts_)\n",
    "# print(classifier.labels_)\n",
    "# print(classifier.labels_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a new classifier\n",
    "\n",
    "We have to use the new clusters to classify the data in the following steps. One possible to solution to use these new clusters is to update the classifier centers and the relevant attributes. Then we can use the classifier's function ```predict``` as before. Moreover, to keep using this classifier to build the next cluster we also need to update the ```labels_``` and ```counts_``` parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_classifier(classifier, new_centers, x):\n",
    "    classifier.cluster_centers_ = new_centers\n",
    "    classifier.n_clusters = len(new_centers)\n",
    "    classifier.labels_, _ = classifier._labels_inertia_minibatch(x)\n",
    "    classifier.counts_ = np.zeros(classifier.n_clusters, dtype=np.int32)\n",
    "    for i in range(classifier.n_clusters):\n",
    "        classifier.counts_[i] = np.sum(classifier.labels_ == i)\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "# classifier = update_classifier(classifier, new_centers, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10;\n",
    "original_classifier = classifier;\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('Epoch %d of %d' % (i + 1, epochs))\n",
    "    print('Number of clusters: %d' % (classifier.n_clusters))\n",
    "    bows = clusters_bow(docs, classifier, w2v, printing=True)\n",
    "    keep, split = study_clusters(bows, labels, printing=True)\n",
    "\n",
    "    print('Keeping %d clusters' % (len([x for x in keep if x == True])))\n",
    "    print('Spliting %d clusters' % (len([x for x in split if x == True])))\n",
    "    \n",
    "    next_centers = update_centers(classifier, X, keep, split)\n",
    "    \n",
    "    classifier = update_classifier(classifier, next_centers, X)\n",
    "#     print_clusters(classifier, w2v)  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (words)",
   "language": "python",
   "name": "words"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
